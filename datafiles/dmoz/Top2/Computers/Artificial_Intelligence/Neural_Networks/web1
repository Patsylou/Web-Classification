 
 Maximum Entropy Online Resources send me email at carlos@math.albany.edu http://www.ipp.mpg.de/maxent04/ Max-Planck-Institute Garhing/Munchen July, 25-28 2004. Email: maxent04@ipp.mpg.de MaxEnt2003: http://maxent23.org Jackson Hole, Wyoming. August 3-8, 2003. Email: gerickson@boisestate.edu MaxEnt2002 MaxEnt2001 MaxEnt2000 MaxEnt99 MaxEnt98 MaxEnt97 Joint meeting with ISBA MaxEnt96: Preliminary Announcement http://gsd.is.co.za/maxent MaxEnt95 V. Kashyap's report on MaxEnt95 MaxEnt94 MaxEnt/STA at Cambridge UK Software package from ristad@cs.princeton.edu Perl5 MaxEnt A Perl5 Module for Maximum Entropy Modeling by Hugo WL ter Doest. Also available from CPAN http://www.cpan.org Manfred Matzke's site on MaxEnt. Mostly deconvolution of spectra Maxent Tutorial at Los Alamos by Gedanken. Entropy in by SourceForge's MaxEnt page See also Adwait Ratnaparkhi's U. Penn 1998 Dissertation. Carlos Rodriguez 
 Skip navigation The Neuroinformatics Site.org Logo Home Login Meetings Courses Belgium Maillist Credits Servers Links Archive Contact Welcome to the neuroinf.org website. The International Neuroinformatics Coordinating Facility Belgium joined the INCF on 1/1/07. Number of hits counted since the start of this site on 01 Jun 2000: Local time: Information for organizers of neuroscience meetings/courses in Belgium The portal neuroinformatics.be www.neuroinformatics.be Page last updated on Tuesday, 11-Feb-2014 18:20:11 CET Contact . 
 The Collection of Computer Science Bibliographies Collection Home Up: The Collection of Computer Science Bibliographies Bibliographies on Neural Networks You can add bibliographies and references Query: any author title field year (four digit years) Results as Citation Results in BibTeX 10 results per page 40 results per page 100 results per page 200 results per page sort by score year online papers only Lucene syntax ti (title), au (author), yr #Refs Bibliography Date 7548 Bibliography on the Self-Organizing Map (SOM) and Learning Vector Quantization (LVQ) (2005) 2621 Neural Network Bibliography (1998) 2558 Bibliography of the Adaptive Systems Group of the GMD (1994) 1743 Bibliography for the IEEE Transactions on Neural Networks (2004) 1526 Bibliography of the paper Simplifying Neural networks by Soft Weight-Sharing (1992) 1344 Bibliography of the journal Neural Computation (2004) 1179 Bibliography for Advances in Neural Information Processing Systems (NIPS) (1998) 1080 Bibliography for the journal Neural Networks (2004) 1041 Bibliography of the book Pattern Recognition and Neural Networks (1996) 1005 Bibliography on Neural Networks (1994) 642 Ensemble Learning (2007) 617 Bibliography on neural networks (2012) 533 Bibliography on neural networks (1994) 383 Bibliography of the Systems Biophysics Group at the University of Bochum (2008) 383 Bibliography on genetic algorithms, neural networks and their combination (1995) 375 Bibliography on Neurofuzzy Systems (1995) 339 Bibliography for the journal Neural Processing Letters (2004) 329 Bibliography on Neural Networks (1993) 329 Bibliography on Evolutionary Design of Neural Architectures (1994) 311 Bibliography on Parallel Simulation of Neural Networks and Parallel Processing in general (1993) 308 Bibliography on constructive algorithms for neural networks (1998) 308 Bibliography on Recurrent Neural Networks (1998) 250 Bibliography on fuzzy logic and neural networks (1996) 203 Bibliography on Reinforcement Learning (1993) 196 Bibliography on Mixture of Experts (1997) 194 Publications of the Autonomous Systems Group at the University of Amsterdam (2001) 182 Bibliography on Invariances in Neural Systems (2005) 167 Bibliography on natural language processing and neural networks (1995) 167 A Bibliography of Connectionist Models of Music (1998) 119 A Short Bibliography of Connectionist Systems for Temporal Behavior (1990) 115 Bibliography on Cortical Map Formation (2000) 108 Bibliography on the Mapping of Neural Networks (1994) 86 Bibliography on Invariant Pattern Recognition with Neural Networks (1993) 79 Bibliography of Fault Tolerance related Neural Network literature (1993) 61 Bibliography on Adaptive Resonance Theory (ART) (2000) 55 Bibliography of Neural Networks (1991) 47 Bibliography on Principal Component Analysis (PCA) Neural Networks (1994) 33 Bibliography of the Neural Network Group at IDIAP (1996) 14 Bibliography of IEEE Transactions on Neural Networks (1991) 2 11th Joint Symposium on Neural Computation (2004) 28580 Total number of references in this section Other Bibliographies on Neural Networks Copyright Paul Ortyl comments liinwwwa@ira.uka.de 
 EVOLUTIONARY DESIGN OF NEURAL ARCHITECTURES Vasant Honavar at Evolutionary Design of Neural Architectures Resource-List GANN Electronic Mailing List majordomo@cs.iastate.edu subscribe gann-list . majordomo@cs.iastate.edu . GANN List Archives here majordomo@cs.iastate.edu with the body index gann-list get gann-list file-name majordomo@cs.iastate.edu . Bibliography and Guide to Literature on EDNA EDNA Bibliography (.bib File) honavar@cs.iastate.edu or balakris@cs.iastate.edu 
 The Gaussian Processes Web Site Books Events Other Web Sites Software Research Papers Books Gaussian Processes for Machine Learning online version . (revised edition), Noel A. C. Cressie, Wiley, 1993 , Grace Wahba, SIAM, 1990 Future and Past Events The A tutorial on Dec. 4th at NIPS 2006 in VanCouver, slides , lecture . The workshop at Bletchley Park, U.K., June 12-13 2006. The The meeting in Sheffield, June 9-10, 2005. Other Web Sites of Related Interest The kernel-machines Wikipedia entry The ai-geostats The web site maintained by Juš Kocijan . Software Andreas Geiger package title author implementation description bcm The Bayesian Committee Machine Anton Schwaighofer matlab and NETLAB fbm Software for Flexible Bayesian Modeling Radford M. Neal C for linux/unix gp-lvm and fgp-lvm A (fast) implementation of Gaussian Process Latent Variable Models Neil D. Lawrence matlab and C gpml Code from the Rasmussen and Williams: Gaussian Processes for Machine Learning book. Carl Edward Rasmussen and Hannes Nickisch matlab and octave describing the toolbox. c++-ivm Sparse approximations based on the Informative Vector Machine Neil D. Lawrence C++ IVM Software in C++ , also includes the null category noise model for semi-supervised learning . BFD Bayesian Fisher's Discriminant software matlab gpor Gaussian Processes for Ordinal Regression Wei Chu C for linux/unix Software implementation of Gaussian Processes for Ordinal Regression . Provides Laplace Approximation, Expectation Propagation and Variational Lower Bound. MCMCstuff MCMC Methods for MLP and GP and Stuff Aki Vehtari matlab and C fbm to matlab for easier development for matlab users. ogp Sparse Online Gaussian Processes Lehel Csató matlab and NETLAB sogp Sparse Online Gaussian Process C++ Library Dan Grollman C++ PhD thesis of Lehel Csató spgp .tgz or .zip Sparse Pseudo-input Gaussian Processes Ed Snelson matlab Implements sparse GP regression as described in Sparse Gaussian Processes using Pseudo-inputs and Flexible and efficient Gaussian process models for machine learning . The SPGP uses gradient-based marginal likelihood optimization to find suitable basis points and kernel hyperparameters in a single joint optimization. tgp Treed Gaussian Processes Robert B. Gramacy C/C++ for R Gramacy 2007 Tpros Gaussian Process Regression David MacKay and Mark Gibbs C GP Demo Octave demonstration of Gaussian process interpolation David MacKay octave This DEMO works fine with octave-2.0 and did not work with 2.1.33. GPClass Matlab code for Gaussian Process Classification David Barber C. K. I. Williams matlab Implements Laplace's approximation as described in Bayesian Classification with Gaussian Processes for binary and multiclass classification. VBGP Variational Bayesian Multinomial Probit Regression with Gaussian Process Priors Mark Girolami Simon Rogers matlab paper Variational Bayesian Multinomial Probit Regression. pyXGPR Relational Gaussian Processes Marion Neumann Python pyXGPR is a library containing code for Gaussian Process Regression (GPR) and relational GP (XGP) Regression. gaussian-process Gaussian process regression Anand Patil Python under development gptk Gaussian Process Tool-Kit Alfredo Kalaitzis R The gptk package implements a general-purpose toolkit for Gaussian process regression with an RBF covariance function. Based on a MATLAB implementation written by Neil D. Lawrence. NETLAB Tom Minka and his lightspeed shares his code www.kernel-machines.org Annotated Bibliography Tutorials Regression Classification Covariance Functions Model Selection Approximations Stats Learning Curves RKHS Reinforcement Learning GP-LVM Applications Other Topics Tutorials Williams 2002 ] over intermediate [ MacKay 1998 ], [ Williams 1999 chapter 45 Comment: Neural Networks and Machine Learning NATO ASI Series Numerical Recipes Comment: chapters Gaussian processes for machine learning International Journal of Neural Systems , 14(2):69-106, 2004. Abstract: Handbook of Brain Theory and Neural Networks Learning in Graphical Models Regression approximation O'Hagan 1978 Williams and Rasmussen 1996 Dependent Gaussian processes Abstract: Abstract: Journal of Statistical Software , 19, 2007. Parameter space exploration with Gaussian process trees 21st International Conference on Machine Learning Abstract: Abstract: Curve fitting and optimal design for prediction Journal of the Royal Statistical Society, Series B Bayesian Inference , volume2B of Behaviourmetrika , 26(1):29-50, 1999. Abstract: Abstract: . The MIT Press, 2002. Abstract: Abstract: . The MIT Press, 2005. Abstract: The equivalent kernel [ Silverman, 1984 . The MIT Press, 1999. Abstract: Abstract: Gaussian regression and optimal finite dimensional linear models Neural Networks and Machine Learning 3 Classification approximation Williams and Barber 1998 or Kuss and Rasmussen 2005 Seeger 2002 Abstract: Abstract: Abstract: Sparse online Gaussian processes Neural Computation , 14(2):641-669, 2002. Variational Gaussian process classifiers IEEE Transactions on Neural Networks , 11(6):1458-1464, 2000. Abstract: Neural Computation , 18(8):1790-1817, 2006. Proceedings of the International Conference in Cmputer Vision Abstract: , 2003. Abstract: Minka 2001 Opper and Winther 2000 , Opper and Winther 2000 Journal of Machine Learning Research , 6:1679-1704, 2005. Abstract: Assessing approximations for Gaussian process classification Abstract: Semi-supervised learning via Gaussian processes Abstract: Regression and classification using Gaussian process priors Bayesian Statistics 6 Abstract: Approximations for binary Gaussian process classification Journal of Machine Learning Research , 9:2035-2078, 2008. Abstract: Gaussian processes for classification: Mean-field algorithms Neural Computation , 12(11):2655-2684, 2000. Abstract: Journal of Machine Learning Research , 7:455-491, 2006. Abstract: Journal of Machine Learning Research , 3:233-269, 2002. Abstract: Sparse Gaussian process classification with multiple classes Abstract: 24th International Conference on Machine Learning , 2007. Abstract: Bayesian classification with Gaussian processes IEEE Transactions on Pattern Analysis and Machine Intelligence code . Abstract: x to one of m classes by predicting P(c| x x x )), where (y)=1/(1+e -y x x Covariance Functions and Properties of Gaussian Processes Abrahamsen 1997 The Geometry of Random Fields The elementary Gaussian processes Annals of Mathematical Statistics Abstract: The intrinsic random functions and their applications Advances in Applied Probability Abstract: Comment: . Nonstationary covariance functions for Gaussian process regression Nonparametric estimation of nonstationary spatial covariance structure Journal of the American Statistical Association Journal of the Royal Statistical Society B Metric spaces and positive definite functions Transactions of the American Mathematical Society Computation with infinite neural networks Neural Computation , 10:1203-1216, 1998. Abstract: Stationary Random Functions Model Selection Comparison of approximate methods for handling hyperparameters Neural Compuration Biometrika Predictive automatic relevance determination by expectation propagation Learning Gaussian process kernels via hierarchical bayes Journal of Machine Learning Research , 3:233-269, 2002. Abstract: Machine Learning , 46(1-3):21-52, 2002. Abstract: Neural Computation . The MIT Press, 1999. Abstract: Journal of the Royal Statistical Society, Series B Approximations sparse Gaussian Processes - Iterative Sparse Approximations Abstract: Abstract: Abstract: Sparse online Gaussian processes Neural Computation , 14(2):641-669, 2002. Finite-dimensional approximation of Gaussian processes Abstract: Abstract: Advances in Neural Information Processing Systems 18 , 2006. Abstract: A Family of Algorithms for Approximate Bayesian Inference Expectation propagation for approximate Bayesian inference Abstract: Regression and classification using Gaussian process priors Bayesian Statistics 6 Abstract: Gaussian processes for classification: Mean-field algorithms Neural Computation , 12(11):2655-2684, 2000. Abstract: Journal of Machine Learning Research , 6:1935-1959, 12 2005. Abstract: Comment: u , stated as O(dnm 2 2 ). . The MIT Press, 2003. Abstract: Abstract: The training cost for a GP has O(N 3 2 Comment: See also the matlab implementation spgp . Abstract: 2 2 Neural Computation , 12(11):2719-2741, 2000. Biometrika References from the Statistics Community kriging Nonparametric Bayesian regression The Annals of Statistics A Bayesian approach to model inadequacy for polynomial regression Biometrika Statistics for Spatial Data Model-based geostatistics (with discussion) Applied Statistics A Bayesian analysis of kriging Technometrics The link between Kriging and thin-plate splines Probability, Statsitics and Optimization Journal of the American Statistical Association Curve fitting and optimal design for prediction Journal of the Royal Statistical Society, Series B , volume 104 of Journal of the Royal Statistical Society B Abstract: latent Gaussian models Spline smoothing: The equivalent variable kernel method Annals of Statistics Journal of the Royal Statistical Society, Series B A kernel approximation to the kriging predictor of a spatial process Ann. Inst. Statist. Math Interpolation of Spatial Data Journal of the Royal Statistical Society, Series B A comparison of kriging with nonparametric regression methods Journal of Multivariate Analysis A Bayesian approach to prediction using polynomials Biometrika Consistency, Learning Curves and Bounds Worst-case bounds for Gaussian process models Advances in Neural Information Processing Systems 18 Abstract: Approximate learning curves for Gaussian processes Neural Computation , 14:1393-1428, 2002. Abstract: IEEE Trans. on Information Theory Annals of Statistics , 36(3):1435-1463, 2008. Upper and lower bounds on the learning curve for Gaussian proccesses Machine Learning , 40:77-102, 2000. Abstract: Reproducing Kernel Hilbert Spaces Theory of reproducing kernels Transactions of the American Mathematical Society Journal of Machine Learning Research , 2:299-312, 2001. Abstract: The Annals of Mathematical Statistics Spline Models for Observational Data Reinforcement Learning Approximate Dynamic Programming with Gaussian Processes Proceedings of the 2008 American Control Conference (ACC 2008) Abstract: , pages 19-24, Bruges, Belgium, April 2008. Gaussian Process Dynamic Programming Neurocomputing , 72(7-9):1508-1524, March 2009. . AAAI Press, 2003. Abstract: 22nd International Conference on Machine Learning Abstract: Abstract: Bayesian policy gradient algorithms Abstract: Bayesian actor-critic algorithms 24th International Conference on Machine Learning Abstract: , pages 742-747, Rome, Italy, April 2007. , pages 352-356, Piscataway, 2003. IEEE. Abstract: Probabilistic Inference for Fast Learning in Control , volume 5323 of , pages 229-242. Springer-Verlag, November 2008. Abstract: Gaussian processes in reinforcement learning Abstract: Gaussian Process Latent Variable Models (GP-LVM) Abstract: occurs. Journal of Machine Learning Research , 6:1783-1816, 2005. Abstract: 24th International Conference on Machine Learning , 2007. Abstract: Gaussian process dynamical models Abstract: Comment: Web page http://www.dgp.toronto.edu/~jmwang/gpdm . Applications JMLR: Workshop and Conference Proceedings , 2007. Abstract: Bioinformatics , 21(16):3385-3393, 2005. Abstract: www.gatsby.ucl.ac.uk/~chuwei/code/gpgenes.tar . Relational learning with Gaussian processes Advances in Neural Information Processing Systems 18 , 2006. Abstract: , pages 192-199. Omnipress, 2008. Prediction on spike data using kernel algorithms Abstract: Abstract: Abstract: , pages 742-747, Rome, Italy, April 2007. Proceedings of the Amarican Control Conference Abstract: Journal of the American Statistical Association Gaussian processes for multiuser detection in cdma receivers Bayesian Statistics 6 Abstract: , 2003. Abstract: Design and analysis of computer experiments Statistical Science Abstract: Biometrics , 63:714-723, 2007. Abstract: Statistics and Computing , 18:267-283, 2008. Pattern Recognition, Proc. 26th DAGM Symposium Abstract: Screening, predicting, and computer experiments Technometrics Comment: Other Topics 25th International Conference on Machine Learning , 2008. Abstract: Journal of Machine Learning Research , 6:1019-1041, 2005. Abstract: Preference learning with Gaussian processe 22nd International Conference on Machine Learning , 2005. Abstract: Abstract: , pages 211-219. Morgan Kaufmann, 2000. Abstract: Abstract: 20th International Conference on Machine Learning , 2003. Abstract: . Abstract: Abstract: Abstract: Journal of Machine Learning Research , 9:235-284, February 2008. Abstract: Irish Signals and Systems Conference Bayes-Hermite quadrature Journal of Statistical Planning and Inference Bayesian Statistics 7 Abstract: . The MIT Press, 2002. Abstract: Abstract: Abstract: Abstract: Comment: Products and sums of tree-structured Gaussian processes and most recently updated on Febriary 23rd, 2011. 
 Artificial neural network From Wikipedia, the free encyclopedia   (Redirected from Neural network ) navigation ,       search "Neural network" redirects here. For networks of living neurons, see Biological neural network . For the journal, see Neural Networks (journal) . Machine learning and data mining Problems Classification Clustering Regression Anomaly detection Association rules Reinforcement learning Structured prediction Feature learning Online learning Semi-supervised learning Grammar induction Supervised learning ( classification • regression ) Decision trees Ensembles ( Bagging , Boosting , Random forest ) k -NN Linear regression Naive Bayes Neural networks Logistic regression Perceptron Support vector machine (SVM) Clustering BIRCH Hierarchical k -means Expectation-maximization (EM) DBSCAN OPTICS Mean-shift Dimensionality reduction Factor analysis CCA ICA LDA NMF PCA t-SNE Structured prediction Graphical models ( CRF , HMM ) Anomaly detection k -NN Local outlier factor ANN Autoencoder SOM Feedforward neural network Restricted Boltzmann machine Deep Learning Theory Bias-variance dilemma Computational learning theory Empirical risk minimization PAC learning Statistical learning VC theory Computer science portal Statistics portal v t e neurons in a brain . Here, each circular node represents an artificial neuron and an arrow represents a connection from the output of one neuron to the input of another. In computer science and related fields, artificial neural networks are computational models inspired by animals' central nervous systems (in particular the brain ) that are capable of machine learning and pattern recognition . They are usually presented as systems of interconnected " neurons " that can compute values from inputs by feeding information through the network. For example, in a neural network for handwriting recognition , a set of input neurons may be activated by the pixels of an input image representing a letter or digit. The activations of these neurons are then passed on, weighted and transformed by some function determined by the network's designer, to other neurons, etc., until finally an output neuron is activated that determines which character was read. Like other machine learning methods, neural networks have been used to solve a wide variety of tasks that are hard to solve using ordinary rule-based programming, including computer vision and speech recognition . Contents 1 Background 2 History 2.1 Recent improvements 2.2 Successes in pattern recognition contests since 2009 3 Models 3.1 Network function 3.2 Learning 3.2.1 Choosing a cost function 3.3 Learning paradigms 3.3.1 Supervised learning 3.3.2 Unsupervised learning 3.3.3 Reinforcement learning 3.4 Learning algorithms 4 Employing artificial neural networks 5 Applications 5.1 Real-life applications 5.2 Neural networks and neuroscience 5.2.1 Types of models 6 Neural network software 7 Types of artificial neural networks 8 Theoretical properties 8.1 Computational power 8.2 Capacity 8.3 Convergence 8.4 Generalization and statistics 8.5 Dynamic properties 9 Criticism 10 Gallery 11 See also 12 References 13 Bibliography 14 External links Background [ edit ] The inspiration for the neural networks came from examination of central nervous systems . In an artificial neural network, simple artificial nodes , called " neurons ", "neurodes", "processing elements" or "units", are connected together to form a network which mimics a biological neural network. There is no single formal definition of what an artificial neural network is. Commonly, a class of statistical models may be called "neural" if they consist of sets of adaptive weights, i.e. numerical parameters that are tuned by a learning algorithm , and are capable of approximating non-linear functions of their inputs. The adaptive weights are conceptually connection strengths between neurons, which are activated during training and prediction. Neural networks are also similar to biological neural networks in performing functions collectively and in parallel by the units, rather than there being a clear delineation of subtasks to which various units are assigned. The term "neural network" usually refers to models employed in statistics , cognitive psychology and artificial intelligence . Neural network models which emulate the central nervous system are part of theoretical neuroscience and computational neuroscience . In modern software implementations of artificial neural networks, the approach inspired by biology has been largely abandoned for a more practical approach based on statistics and signal processing. In some of these systems, neural networks or parts of neural networks (like artificial neurons) form components in larger systems that combine both adaptive and non-adaptive elements. While the more general approach of such systems is more suitable for real-world problem solving, it has little to do with the traditional artificial intelligence connectionist models. What they do have in common, however, is the principle of non-linear, distributed, parallel and local processing and adaptation. Historically, the use of neural networks models marked a paradigm shift in the late eighties from high-level (symbolic) artificial intelligence , characterized by expert systems with knowledge embodied in if-then rules, to low-level (sub-symbolic) machine learning , characterized by knowledge embodied in the parameters of a dynamical system . History [ edit ] Warren McCulloch and Walter Pitts [ 1 ] (1943) created a computational model for neural networks based on mathematics and algorithms. They called this model threshold logic . The model paved the way for neural network research to split into two distinct approaches. One approach focused on biological processes in the brain and the other focused on the application of neural networks to artificial intelligence. In the late 1940s psychologist Donald Hebb [ 2 ] created a hypothesis of learning based on the mechanism of neural plasticity that is now known as Hebbian learning . Hebbian learning is considered to be a 'typical' unsupervised learning rule and its later variants were early models for long term potentiation . These ideas started being applied to computational models in 1948 with Turing's B-type machines . Farley and Clark [ 3 ] (1954) first used computational machines, then called calculators, to simulate a Hebbian network at MIT. Other neural network computational machines were created by Rochester, Holland, Habit, and Duda [ 4 ] (1956). Frank Rosenblatt [ 5 ] (1958) created the perceptron , an algorithm for pattern recognition based on a two-layer learning computer network using simple addition and subtraction. With mathematical notation, Rosenblatt also described circuitry not in the basic perceptron, such as the exclusive-or circuit, a circuit whose mathematical computation could not be processed until after the backpropagation algorithm was created by Paul Werbos [ 6 ] (1975). Neural network research stagnated after the publication of machine learning research by Marvin Minsky and Seymour Papert [ 7 ] (1969). They discovered two key issues with the computational machines that processed neural networks. The first issue was that single-layer neural networks were incapable of processing the exclusive-or circuit. The second significant issue was that computers were not sophisticated enough to effectively handle the long run time required by large neural networks. Neural network research slowed until computers achieved greater processing power. Also key later advances was the backpropagation algorithm which effectively solved the exclusive-or problem (Werbos 1975). [ 6 ] The parallel distributed processing of the mid-1980s became popular under the name connectionism . The text by David E. Rumelhart and James McClelland [ 8 ] (1986) provided a full exposition on the use of connectionism in computers to simulate neural processes. Neural networks, as used in artificial intelligence, have traditionally been viewed as simplified models of neural processing in the brain, even though the relation between this model and brain biological architecture is debated, as it is not clear to what degree artificial neural networks mirror brain function. [ 9 ] In the 1990s, neural networks were overtaken in popularity in machine learning by support vector machines and other, much simpler methods such as linear classifiers . Renewed interest in neural nets was sparked in the 2000s by the advent of deep learning . Recent improvements [ edit ] Biophysical models, such as BCM theory , have been important in understanding mechanisms for synaptic plasticity , and have had applications in both computer science and neuroscience. Research is ongoing in understanding the computational algorithms used in the brain, with some recent biological evidence for radial basis networks and neural backpropagation as mechanisms for processing data. Computational devices have been created in CMOS, for both biophysical simulation and neuromorphic computing . More recent efforts show promise for creating nanodevices [ 10 ] for very large scale principal components analyses and convolution . If successful, these efforts could usher in a new era of neural computing [ 11 ] that is a step beyond digital computing, because it depends on learning rather than programming and because it is fundamentally analog rather than digital even though the first instantiations may in fact be with CMOS digital devices. Between 2009 and 2012, the recurrent neural networks and deep feedforward neural networks developed in the research group of Jürgen Schmidhuber at the Swiss AI Lab IDSIA have won eight international competitions in pattern recognition and machine learning . [ 12 ] For example, multi-dimensional long short term memory (LSTM) [ 13 ] [ 14 ] won three competitions in connected handwriting recognition at the 2009 International Conference on Document Analysis and Recognition (ICDAR), without any prior knowledge about the three different languages to be learned. Variants of the back-propagation algorithm as well as unsupervised methods by Geoff Hinton and colleagues at the University of Toronto [ 15 ] [ 16 ] can be used to train deep, highly nonlinear neural architectures similar to the 1980 Neocognitron by Kunihiko Fukushima , [ 17 ] and the "standard architecture of vision", [ 18 ] inspired by the simple and complex cells identified by David H. Hubel and Torsten Wiesel in the primary visual cortex . Deep learning feedforward networks, such as convolutional neural networks , alternate convolutional layers and max-pooling layers, topped by several pure classification layers. Fast GPU -based implementations of this approach have won several pattern recognition contests, including the IJCNN 2011 Traffic Sign Recognition Competition [ 19 ] and the ISBI 2012 Segmentation of Neuronal Structures in Electron Microscopy Stacks challenge. [ 20 ] Such neural networks also were the first artificial pattern recognizers to achieve human-competitive or even superhuman performance [ 21 ] on benchmarks such as traffic sign recognition (IJCNN 2012), or the MNIST handwritten digits problem of Yann LeCun and colleagues at NYU . Successes in pattern recognition contests since 2009 [ edit ] Between 2009 and 2012, the recurrent neural networks and deep feedforward neural networks developed in the research group of Jürgen Schmidhuber at the Swiss AI Lab IDSIA have won eight international competitions in pattern recognition and machine learning . [ 22 ] For example, the bi-directional and multi-dimensional long short term memory (LSTM) [ 23 ] [ 24 ] of Alex Graves et al. won three competitions in connected handwriting recognition at the 2009 International Conference on Document Analysis and Recognition (ICDAR), without any prior knowledge about the three different languages to be learned. Fast GPU -based implementations of this approach by Dan Ciresan and colleagues at IDSIA have won several pattern recognition contests, including the IJCNN 2011 Traffic Sign Recognition Competition, [ 25 ] the ISBI 2012 Segmentation of Neuronal Structures in Electron Microscopy Stacks challenge, [ 20 ] and others. Their neural networks also were the first artificial pattern recognizers to achieve human-competitive or even superhuman performance [ 21 ] on important benchmarks such as traffic sign recognition (IJCNN 2012), or the MNIST handwritten digits problem of Yann LeCun at NYU . Deep, highly nonlinear neural architectures similar to the 1980 neocognitron by Kunihiko Fukushima [ 17 ] and the "standard architecture of vision" [ 18 ] can also be pre-trained by unsupervised methods [ 26 ] [ 27 ] of Geoff Hinton 's lab at University of Toronto . A team from this lab won a 2012 contest sponsored by Merck to design software to help find molecules that might lead to new drugs. [ 28 ] Models [ edit ] Neural network models in artificial intelligence are usually referred to as artificial neural networks (ANNs); these are essentially simple mathematical models defining a function or a distribution over or both and , but sometimes models are also intimately associated with a particular learning algorithm or learning rule. A common use of the phrase ANN model really means the definition of a class of such functions (where members of the class are obtained by varying parameters, connection weights, or specifics of the architecture such as the number of neurons or their connectivity). Network function [ edit ] See also: Graphical models The word network in the term 'artificial neural network' refers to the inter–connections between the neurons in the different layers of each system. An example system has three layers. The first layer has input neurons which send data via synapses to the second layer of neurons, and then via more synapses to the third layer of output neurons. More complex systems will have more layers of neurons with some having increased layers of input neurons and output neurons. The synapses store parameters called "weights" that manipulate the data in the calculations. An ANN is typically defined by three types of parameters: The interconnection pattern between the different layers of neurons The learning process for updating the weights of the interconnections The activation function that converts a neuron's weighted input to its output activation. Mathematically, a neuron's network function is defined as a composition of other functions , which can further be defined as a composition of other functions. This can be conveniently represented as a network structure, with arrows depicting the dependencies between variables. A widely used type of composition is the nonlinear weighted sum , where , where (commonly referred to as the activation function [ 29 ] ) is some predefined function, such as the hyperbolic tangent . It will be convenient for the following to refer to a collection of functions as simply a vector . This figure depicts such a decomposition of , with dependencies between variables indicated by arrows. These can be interpreted in two ways. The first view is the functional view: the input is transformed into a 3-dimensional vector , which is then transformed into a 2-dimensional vector , which is finally transformed into . This view is most commonly encountered in the context of optimization . The second view is the probabilistic view: the random variable depends upon the random variable , which depends upon , which depends upon the random variable . This view is most commonly encountered in the context of graphical models . The two views are largely equivalent. In either case, for this particular network architecture, the components of individual layers are independent of each other (e.g., the components of are independent of each other given their input ). This naturally enables a degree of parallelism in the implementation. Networks such as the previous one are commonly called feedforward , because their graph is a directed acyclic graph . Networks with cycles are commonly called recurrent . Such networks are commonly depicted in the manner shown at the top of the figure, where is shown as being dependent upon itself. However, an implied temporal dependence is not shown. Learning [ edit ] What has attracted the most interest in neural networks is the possibility of learning . Given a specific task to solve, and a class of functions , learning means using a set of observations to find which solves the task in some optimal sense. This entails defining a cost function such that, for the optimal solution , – i.e., no solution has a cost less than the cost of the optimal solution (see Mathematical optimization ). The cost function is an important concept in learning, as it is a measure of how far away a particular solution is from an optimal solution to the problem to be solved. Learning algorithms search through the solution space to find a function that has the smallest possible cost. For applications where the solution is dependent on some data, the cost must necessarily be a function of the observations , otherwise we would not be modelling anything related to the data. It is frequently defined as a statistic to which only approximations can be made. As a simple example, consider the problem of finding the model , which minimizes , for data pairs drawn from some distribution . In practical situations we would only have samples from and thus, for the above example, we would only minimize . Thus, the cost is minimized over a sample of the data rather than the entire data set. When some form of online machine learning must be used, where the cost is partially minimized as each new example is seen. While online machine learning is often used when is fixed, it is most useful in the case where the distribution changes slowly over time. In neural network methods, some form of online machine learning is frequently used for finite datasets. See also: Mathematical optimization , Estimation theory ,and Machine learning Choosing a cost function [ edit ] While it is possible to define some arbitrary ad hoc cost function, frequently a particular cost will be used, either because it has desirable properties (such as convexity ) or because it arises naturally from a particular formulation of the problem (e.g., in a probabilistic formulation the posterior probability of the model can be used as an inverse cost). Ultimately, the cost function will depend on the desired task. An overview of the three (3) main categories of learning tasks is provided below: Learning paradigms [ edit ] There are three major learning paradigm, each corresponding to a particular abstract learning task. These are supervised learning , unsupervised learning and reinforcement learning . Supervised learning [ edit ] In supervised learning , we are given a set of example pairs and the aim is to find a function in the allowed class of functions that matches the examples. In other words, we wish to infer the mapping implied by the data; the cost function is related to the mismatch between our mapping and the data and it implicitly contains prior knowledge about the problem domain. A commonly used cost is the mean-squared error , which tries to minimize the average squared error between the network's output, f(x), and the target value y over all the example pairs. When one tries to minimize this cost using gradient descent for the class of neural networks called multilayer perceptrons , one obtains the common and well-known backpropagation algorithm for training neural networks. Tasks that fall within the paradigm of supervised learning are pattern recognition (also known as classification) and regression (also known as function approximation). The supervised learning paradigm is also applicable to sequential data (e.g., for speech and gesture recognition). This can be thought of as learning with a "teacher," in the form of a function that provides continuous feedback on the quality of solutions obtained thus far. Unsupervised learning [ edit ] In unsupervised learning , some data is given and the cost function to be minimized, that can be any function of the data and the network's output, . The cost function is dependent on the task (what we are trying to model) and our a priori assumptions (the implicit properties of our model, its parameters and the observed variables). As a trivial example, consider the model where is a constant and the cost . Minimizing this cost will give us a value of that is equal to the mean of the data. The cost function can be much more complicated. Its form depends on the application: for example, in compression it could be related to the mutual information between and , whereas in statistical modeling, it could be related to the posterior probability of the model given the data. (Note that in both of those examples those quantities would be maximized rather than minimized). Tasks that fall within the paradigm of unsupervised learning are in general estimation problems; the applications include clustering , the estimation of statistical distributions , compression and filtering . Reinforcement learning [ edit ] In reinforcement learning , data are usually not given, but generated by an agent's interactions with the environment. At each point in time , the agent performs an action and the environment generates an observation and an instantaneous cost , according to some (usually unknown) dynamics. The aim is to discover a policy for selecting actions that minimizes some measure of a long-term cost; i.e., the expected cumulative cost. The environment's dynamics and the long-term cost for each policy are usually unknown, but can be estimated. More formally the environment is modelled as a Markov decision process (MDP) with states and actions with the following probability distributions: the instantaneous cost distribution , the observation distribution and the transition , while a policy is defined as conditional distribution over actions given the observations. Taken together, the two then define a Markov chain (MC). The aim is to discover the policy that minimizes the cost; i.e., the MC for which the cost is minimal. ANNs are frequently used in reinforcement learning as part of the overall algorithm. [ 30 ] [ 31 ] Dynamic programming has been coupled with ANNs (Neuro dynamic programming) by Bertsekas and Tsitsiklis [ 32 ] and applied to multi-dimensional nonlinear problems such as those involved in vehicle routing , [ 33 ] natural resources management [ 34 ] [ 35 ] or medicine [ 36 ] because of the ability of ANNs to mitigate losses of accuracy even when reducing the discretization grid density for numerically approximating the solution of the original control problems. Tasks that fall within the paradigm of reinforcement learning are control problems, games and other sequential decision making tasks. See also: dynamic programming and stochastic control Learning algorithms [ edit ] Training a neural network model essentially means selecting one model from the set of allowed models (or, in a Bayesian framework, determining a distribution over the set of allowed models) that minimizes the cost criterion. There are numerous algorithms available for training neural network models; most of them can be viewed as a straightforward application of optimization theory and statistical estimation . Most of the algorithms used in training artificial neural networks employ some form of gradient descent . This is done by simply taking the derivative of the cost function with respect to the network parameters and then changing those parameters in a gradient-related direction. Evolutionary methods , [ 37 ] gene expression programming , [ 38 ] simulated annealing , [ 39 ] expectation-maximization , non-parametric methods and particle swarm optimization [ 40 ] are some commonly used methods for training neural networks. See also: machine learning Employing artificial neural networks [ edit ] Perhaps the greatest advantage of ANNs is their ability to be used as an arbitrary function approximation mechanism that 'learns' from observed data. However, using them is not so straightforward, and a relatively good understanding of the underlying theory is essential. Choice of model: This will depend on the data representation and the application. Overly complex models tend to lead to problems with learning. Learning algorithm: There are numerous trade-offs between learning algorithms. Almost any algorithm will work well with the correct hyperparameters for training on a particular fixed data set. However, selecting and tuning an algorithm for training on unseen data requires a significant amount of experimentation. Robustness: If the model, cost function and learning algorithm are selected appropriately the resulting ANN can be extremely robust. With the correct implementation, ANNs can be used naturally in online learning and large data set applications. Their simple implementation and the existence of mostly local dependencies exhibited in the structure allows for fast, parallel implementations in hardware. Applications [ edit ] The utility of artificial neural network models lies in the fact that they can be used to infer a function from observations. This is particularly useful in applications where the complexity of the data or task makes the design of such a function by hand impractical. Real-life applications [ edit ] The tasks artificial neural networks are applied to tend to fall within the following broad categories: Function approximation , or regression analysis , including time series prediction , fitness approximation and modeling. Classification , including pattern and sequence recognition, novelty detection and sequential decision making. Data processing , including filtering, clustering, blind source separation and compression. Robotics , including directing manipulators, prosthesis . Control , including Computer numerical control . Application areas include the system identification and control (vehicle control, process control, natural resources management), quantum chemistry, [ 41 ] game-playing and decision making (backgammon, chess, poker ), pattern recognition (radar systems, face identification, object recognition and more), sequence recognition (gesture, speech, handwritten text recognition), medical diagnosis, financial applications (e.g. automated trading systems ), data mining (or knowledge discovery in databases, "KDD"), visualization and e-mail spam filtering. Artificial neural networks have also been used to diagnose several cancers. An ANN based hybrid lung cancer detection system named HLND improves the accuracy of diagnosis and the speed of lung cancer radiology. [ 42 ] These networks have also been used to diagnose prostate cancer. The diagnoses can be used to make specific models taken from a large group of patients compared to information of one given patient. The models do not depend on assumptions about correlations of different variables. Colorectal cancer has also been predicted using the neural networks. Neural networks could predict the outcome for a patient with colorectal cancer with more accuracy than the current clinical methods. After training, the networks could predict multiple patient outcomes from unrelated institutions. [ 43 ] Neural networks and neuroscience [ edit ] Theoretical and computational neuroscience is the field concerned with the theoretical analysis and the computational modeling of biological neural systems. Since neural systems are intimately related to cognitive processes and behavior, the field is closely related to cognitive and behavioral modeling. The aim of the field is to create models of biological neural systems in order to understand how biological systems work. To gain this understanding, neuroscientists strive to make a link between observed biological processes (data), biologically plausible mechanisms for neural processing and learning ( biological neural network models) and theory (statistical learning theory and information theory ). Types of models [ edit ] Many models are used in the field, defined at different levels of abstraction and modeling different aspects of neural systems. They range from models of the short-term behavior of individual neurons , models of how the dynamics of neural circuitry arise from interactions between individual neurons and finally to models of how behavior can arise from abstract neural modules that represent complete subsystems. These include models of the long-term, and short-term plasticity, of neural systems and their relations to learning and memory from the individual neuron to the system level. Neural network software [ edit ] Main article: Neural network software Neural network software is used to simulate , research , develop and apply artificial neural networks, biological neural networks and, in some cases, a wider array of adaptive systems . Types of artificial neural networks [ edit ] Main article: Types of artificial neural networks Artificial neural network types vary from those with only one or two layers of single direction logic, to complicated multi–input many directional feedback loops and layers. On the whole, these systems use algorithms in their programming to determine control and organization of their functions. Some may be as simple as a one-neuron layer with an input and an output, and others can mimic complex systems such as dANN , which can mimic chromosomal DNA through sizes at the cellular level, into artificial organisms and simulate reproduction, mutation and population sizes. [ 44 ] Most systems use "weights" to change the parameters of the throughput and the varying connections to the neurons. Artificial neural networks can be autonomous and learn by input from outside "teachers" or even self-teaching from written-in rules. Theoretical properties [ edit ] Computational power [ edit ] The multi-layer perceptron (MLP) is a universal function approximator, as proven by the Cybenko theorem . However, the proof is not constructive regarding the number of neurons required or the settings of the weights. Work by Hava Siegelmann and Eduardo D. Sontag has provided a proof that a specific recurrent architecture with rational valued weights (as opposed to full precision real number -valued weights) has the full power of a Universal Turing Machine [ 45 ] using a finite number of neurons and standard linear connections. They have further shown that the use of irrational values for weights results in a machine with super-Turing power. [ citation needed ] Capacity [ edit ] Artificial neural network models have a property called 'capacity', which roughly corresponds to their ability to model any given function. It is related to the amount of information that can be stored in the network and to the notion of complexity. Convergence [ edit ] Nothing can be said in general about convergence since it depends on a number of factors. Firstly, there may exist many local minima. This depends on the cost function and the model. Secondly, the optimization method used might not be guaranteed to converge when far away from a local minimum. Thirdly, for a very large amount of data or parameters, some methods become impractical. In general, it has been found that theoretical guarantees regarding convergence are an unreliable guide to practical application. [ citation needed ] Generalization and statistics [ edit ] In applications where the goal is to create a system that generalizes well in unseen examples, the problem of over-training has emerged. This arises in convoluted or over-specified systems when the capacity of the network significantly exceeds the needed free parameters. There are two schools of thought for avoiding this problem: The first is to use cross-validation and similar techniques to check for the presence of overtraining and optimally select hyperparameters such as to minimize the generalization error. The second is to use some form of regularization . This is a concept that emerges naturally in a probabilistic (Bayesian) framework, where the regularization can be performed by selecting a larger prior probability over simpler models; but also in statistical learning theory, where the goal is to minimize over two quantities: the 'empirical risk' and the 'structural risk', which roughly corresponds to the error over the training set and the predicted error in unseen data due to overfitting. Supervised neural networks that use an MSE cost function can use formal statistical methods to determine the confidence of the trained model. The MSE on a validation set can be used as an estimate for variance. This value can then be used to calculate the confidence interval of the output of the network, assuming a normal distribution . A confidence analysis made this way is statistically valid as long as the output probability distribution stays the same and the network is not modified. By assigning a softmax activation function , a generalization of the logistic function , on the output layer of the neural network (or a softmax component in a component-based neural network) for categorical target variables, the outputs can be interpreted as posterior probabilities. This is very useful in classification as it gives a certainty measure on classifications. The softmax activation function is: Dynamic properties [ edit ] This article needs attention from an expert in Technology . Please add a reason or a talk parameter to this template to explain the issue with the article. WikiProject Technology (or its Portal ) may be able to help recruit an expert. (November 2008) Various techniques originally developed for studying disordered magnetic systems (i.e., the spin glass ) have been successfully applied to simple neural network architectures, such as the Hopfield network . Influential work by E. Gardner and B. Derrida has revealed many interesting properties about perceptrons with real-valued synaptic weights, while later work by W. Krauth and M. Mezard has extended these principles to binary-valued synapses. Criticism [ edit ] A common criticism of neural networks, particularly in robotics, is that they require a large diversity of training for real-world operation. This is not surprising, since any learning machine needs sufficient representative examples in order to capture the underlying structure that allows it to generalize to new cases. Dean Pomerleau, in his research presented in the paper "Knowledge-based Training of Artificial Neural Networks for Autonomous Robot Driving," uses a neural network to train a robotic vehicle to drive on multiple types of roads (single lane, multi-lane, dirt, etc.). A large amount of his research is devoted to (1) extrapolating multiple training scenarios from a single training experience, and (2) preserving past training diversity so that the system does not become overtrained (if, for example, it is presented with a series of right turns – it should not learn to always turn right). These issues are common in neural networks that must decide from amongst a wide variety of responses, but can be dealt with in several ways, for example by randomly shuffling the training examples, by using a numerical optimization algorithm that does not take too large steps when changing the network connections following an example, or by grouping examples in so-called mini-batches. A. K. Dewdney , a former Scientific American columnist, wrote in 1997, "Although neural nets do solve a few toy problems, their powers of computation are so limited that I am surprised anyone takes them seriously as a general problem-solving tool." (Dewdney, p.82) Arguments for Dewdney's position are that to implement large and effective software neural networks, much processing and storage resources need to be committed. While the brain has hardware tailored to the task of processing signals through a graph of neurons, simulating even a most simplified form on Von Neumann technology may compel a neural network designer to fill many millions of database rows for its connections – which can consume vast amounts of computer memory and hard disk space. Furthermore, the designer of neural network systems will often need to simulate the transmission of signals through many of these connections and their associated neurons – which must often be matched with incredible amounts of CPU processing power and time. While neural networks often yield effective programs, they too often do so at the cost of efficiency (they tend to consume considerable amounts of time and money). Arguments against Dewdney's position are that neural nets have been successfully used to solve many complex and diverse tasks, ranging from autonomously flying aircraft [ 46 ] to detecting credit card fraud . [ citation needed ] Technology writer Roger Bridgman commented on Dewdney's statements about neural nets: Neural networks, for instance, are in the dock not only because they have been hyped to high heaven, (what hasn't?) but also because you could create a successful net without understanding how it worked: the bunch of numbers that captures its behaviour would in all probability be "an opaque, unreadable table...valueless as a scientific resource". In spite of his emphatic declaration that science is not technology, Dewdney seems here to pillory neural nets as bad science when most of those devising them are just trying to be good engineers. An unreadable table that a useful machine could read would still be well worth having. [ 47 ] In response to this kind of criticism, one should note that although it is true that analyzing what has been learned by an artificial neural network is difficult, it is much easier to do so than to analyze what has been learned by a biological neural network. Furthermore, researchers involved in exploring learning algorithms for neural networks are gradually uncovering generic principles which allow a learning machine to be successful. For example, Bengio and LeCun (2007) wrote an article regarding local vs non-local learning, as well as shallow vs deep architecture. [ 48 ] Some other criticisms came from believers of hybrid models (combining neural networks and symbolic approaches). They advocate the intermix of these two approaches and believe that hybrid models can better capture the mechanisms of the human mind (Sun and Bookman, 1990). Gallery [ edit ] A single-layer feedforward artificial neural network. Arrows originating from are omitted for clarity. There are p inputs to this network and q outputs. In this system, the value of the qth output, would be calculated as A two-layer feedforward artificial neural network. See also [ edit ] 20Q ADALINE Adaptive resonance theory Artificial life Associative memory Autoencoder Backpropagation BEAM robotics Biological cybernetics Biologically inspired computing Blue brain Cerebellar Model Articulation Controller Cognitive architecture Cognitive science Connectionist expert system Connectomics Cultured neuronal networks Digital morphogenesis Encog Fuzzy logic Gene expression programming Genetic algorithm Group method of data handling Habituation In Situ Adaptive Tabulation Memristor Multilinear subspace learning Neuroevolution Neural gas Neural network software Neuroscience Ni1000 chip Nonlinear system identification Optical neural network Parallel Constraint Satisfaction Processes Parallel distributed processing Radial basis function network Recurrent neural networks Self-organizing map Systolic array Tensor product network Time delay neural network (TDNN) References [ edit ] ^ McCulloch, Warren; Walter Pitts (1943). "A Logical Calculus of Ideas Immanent in Nervous Activity". Bulletin of Mathematical Biophysics 5 (4): 115–133. doi : 10.1007/BF02478259 . Cite uses deprecated parameters ( help ) ^ Hebb, Donald (1949). The Organization of Behavior . New York: Wiley. ^ Farley, B; W.A. Clark (1954). "Simulation of Self-Organizing Systems by Digital Computer". IRE Transactions on Information Theory 4 (4): 76–84. doi : 10.1109/TIT.1954.1057468 . Cite uses deprecated parameters ( help ) ^ Rochester, N.; J.H. Holland, L.H. Habit, and W.L. Duda (1956). "Tests on a cell assembly theory of the action of the brain, using a large digital computer". IRE Transactions on Information Theory 2 (3): 80–93. doi : 10.1109/TIT.1956.1056810 . Cite uses deprecated parameters ( help ) ^ Rosenblatt, F. (1958). "The Perceptron: A Probalistic Model For Information Storage And Organization In The Brain". Psychological Review 65 (6): 386–408. doi : 10.1037/h0042519 . PMID 13602029 . ^ a b Werbos, P.J. (1975). Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences . ^ Minsky, M.; S. Papert (1969). An Introduction to Computational Geometry . MIT Press. ISBN 0-262-63022-2 . Cite uses deprecated parameters ( help ) ^ Rumelhart, D.E; James McClelland (1986). Parallel Distributed Processing: Explorations in the Microstructure of Cognition . Cambridge: MIT Press. Cite uses deprecated parameters ( help ) ^ Russell, Ingrid. "Neural Networks Module" . Retrieved 2012 . ^ Yang, J. J.; Pickett, M. D.; Li, X. M.; Ohlberg, D. A. A.; Stewart, D. R.; Williams, R. S. Nat. Nanotechnol. 2008, 3, 429–433. ^ Strukov, D. B.; Snider, G. S.; Stewart, D. R.; Williams, R. S. Nature 2008, 453, 80–83. ^ http://www.kurzweilai.net/how-bio-inspired-deep-learning-keeps-winning-competitions 2012 Kurzweil AI Interview with Jürgen Schmidhuber on the eight competitions won by his Deep Learning team 2009–2012 ^ Graves, Alex; and Schmidhuber, Jürgen; Offline Handwriting Recognition with Multidimensional Recurrent Neural Networks , in Bengio, Yoshua; Schuurmans, Dale; Lafferty, John; Williams, Chris K. I.; and Culotta, Aron (eds.), Advances in Neural Information Processing Systems 22 (NIPS'22), December 7th–10th, 2009, Vancouver, BC , Neural Information Processing Systems (NIPS) Foundation, 2009, pp. 545–552 ^ A. Graves, M. Liwicki, S. Fernandez, R. Bertolami, H. Bunke, J. Schmidhuber. A Novel Connectionist System for Improved Unconstrained Handwriting Recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 31, no. 5, 2009. ^ http://www.scholarpedia.org/article/Deep_belief_networks / ^ Hinton, G. E. ; Osindero, S.; Teh, Y. (2006). "A fast learning algorithm for deep belief nets" . Neural Computation 18 (7): 1527–1554. doi : 10.1162/neco.2006.18.7.1527 . PMID 16764513 . ^ a b Fukushima, K. (1980). "Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position". Biological Cybernetics 36 (4): 93–202. doi : 10.1007/BF00344251 . PMID 7370364 . ^ a b M Riesenhuber, T Poggio . Hierarchical models of object recognition in cortex. Nature neuroscience, 1999. ^ D. C. Ciresan, U. Meier, J. Masci, J. Schmidhuber. Multi-Column Deep Neural Network for Traffic Sign Classification. Neural Networks, 2012. ^ a b D. Ciresan, A. Giusti, L. Gambardella, J. Schmidhuber. Deep Neural Networks Segment Neuronal Membranes in Electron Microscopy Images. In Advances in Neural Information Processing Systems (NIPS 2012), Lake Tahoe, 2012. ^ a b D. C. Ciresan, U. Meier, J. Schmidhuber . Multi-column Deep Neural Networks for Image Classification. IEEE Conf. on Computer Vision and Pattern Recognition CVPR 2012. ^ 2012 Kurzweil AI Interview with Jürgen Schmidhuber on the eight competitions won by his Deep Learning team 2009–2012 ^ Graves, Alex; and Schmidhuber, Jürgen; Offline Handwriting Recognition with Multidimensional Recurrent Neural Networks , in Bengio, Yoshua; Schuurmans, Dale; Lafferty, John; Williams, Chris K. I.; and Culotta, Aron (eds.), Advances in Neural Information Processing Systems 22 (NIPS'22), 7–10 December 2009, Vancouver, BC , Neural Information Processing Systems (NIPS) Foundation, 2009, pp. 545–552. ^ A. Graves, M. Liwicki, S. Fernandez, R. Bertolami, H. Bunke, J. Schmidhuber . A Novel Connectionist System for Improved Unconstrained Handwriting Recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 31, no. 5, 2009. ^ D. C. Ciresan, U. Meier, J. Masci, J. Schmidhuber . Multi-Column Deep Neural Network for Traffic Sign Classification. Neural Networks, 2012. ^ Deep belief networks at Scholarpedia. ^ Hinton, G. E. ; Osindero, S.; Teh, Y. W. (2006). "A Fast Learning Algorithm for Deep Belief Nets" . Neural Computation 18 (7): 1527–1554. doi : 10.1162/neco.2006.18.7.1527 . PMID 16764513 . edit ^ John Markoff (November 23, 2012). "Scientists See Promise in Deep-Learning Programs" . New York Times . ^ "The Machine Learning Dictionary" . ^ Dominic, S., Das, R., Whitley, D., Anderson, C. (July 1991). "Genetic reinforcement learning for neural networks" . IJCNN-91-Seattle International Joint Conference on Neural Networks . IJCNN-91-Seattle International Joint Conference on Neural Networks. Seattle, Washington, USA: IEEE. doi : 10.1109/IJCNN.1991.155315 . ISBN 0-7803-0164-1 . Retrieved 29 July 2012 . ^ Hoskins, J.C.; Himmelblau, D.M. (1992). "Process control via artificial neural networks and reinforcement learning". Computers Chemical Engineering 16 (4): 241–251. doi : 10.1016/0098-1354(92)80045-B . Cite uses deprecated parameters ( help ) ^ Bertsekas, D.P., Tsitsiklis, J.N. (1996). Neuro-dynamic programming . Athena Scientific. p.512. ISBN 1-886529-10-8 . ^ Secomandi, Nicola (2000). "Comparing neuro-dynamic programming algorithms for the vehicle routing problem with stochastic demands". Computers Operations Research 27 (11–12): 1201–1225. doi : 10.1016/S0305-0548(99)00146-X . ^ de Rigo, D., Rizzoli, A. E., Soncini-Sessa, R., Weber, E., Zenesi, P. (2001). "Neuro-dynamic programming for the efficient management of reservoir networks" . Proceedings of MODSIM 2001, International Congress on Modelling and Simulation . MODSIM 2001, International Congress on Modelling and Simulation . Canberra, Australia: Modelling and Simulation Society of Australia and New Zealand. doi : 10.5281/zenodo.7481 . ISBN 0-867405252 . Retrieved 29 July 2012 . ^ Damas, M., Salmeron, M., Diaz, A., Ortega, J., Prieto, A., Olivares, G. (2000). "Genetic algorithms and neuro-dynamic programming: application to water supply networks" . Proceedings of 2000 Congress on Evolutionary Computation . 2000 Congress on Evolutionary Computation. La Jolla, California, USA: IEEE. doi : 10.1109/CEC.2000.870269 . ISBN 0-7803-6375-2 . Retrieved 29 July 2012 . ^ Deng, Geng; Ferris, M.C. (2008). "Neuro-dynamic programming for fractionated radiotherapy planning". Springer Optimization and Its Applications 12 : 47–70. doi : 10.1007/978-0-387-73299-2_3 . Cite uses deprecated parameters ( help ) ^ de Rigo, D., Castelletti, A., Rizzoli, A.E., Soncini-Sessa, R., Weber, E. (January 2005). "A selective improvement technique for fastening Neuro-Dynamic Programming in Water Resources Network Management" . In Pavel Zítek. Proceedings of the 16th IFAC World Congress – IFAC-PapersOnLine . 16th IFAC World Congress 16 . Prague, Czech Republic: IFAC. doi : 10.3182/20050703-6-CZ-1902.02172 . ISBN 978-3-902661-75-3 . Retrieved 30 December 2011 . ^ Ferreira, C. (2006). "Designing Neural Networks Using Gene Expression Programming" . In A. Abraham, B. de Baets, M. Köppen, and B. Nickolay, eds., Applied Soft Computing Technologies: The Challenge of Complexity, pages 517–536, Springer-Verlag. ^ Da, Y., Xiurun, G. (July 2005). T. Villmann, ed. "An improved PSO-based ANN with simulated annealing technique". New Aspects in Neurocomputing: 11th European Symposium on Artificial Neural Networks . Elsevier. doi : 10.1016/j.neucom.2004.07.002 . Retrieved 30 December 2011 . ^ Wu, J., Chen, E. (May 2009). Wang, H., Shen, Y., Huang, T., Zeng, Z., ed. "A Novel Nonparametric Regression Ensemble for Rainfall Forecasting Using Particle Swarm Optimization Technique Coupled with Artificial Neural Network". 6th International Symposium on Neural Networks, ISNN 2009 . Springer. doi : 10.1007/978-3-642-01513-7_6 . ISBN 978-3-642-01215-0 . Retrieved 1 January 2012 . ^ Roman M. Balabin, Ekaterina I. Lomakina (2009). "Neural network approach to quantum-chemistry data: Accurate prediction of density functional theory energies". J. Chem. Phys. 131 (7): 074104. doi : 10.1063/1.3206326 . PMID 19708729 . ^ Ganesan, N. "Application of Neural Networks in Diagnosing Cancer Disease Using Demographic Data" . International Journal of Computer Applications. ^ Bottaci, Leonardo. "Artificial Neural Networks Applied to Outcome Prediction for Colorectal Cancer Patients in Separate Institutions" . The Lancet. ^ "DANN:Genetic Wavelets" . dANN project. Archived from the original on 21 August 2010 . Retrieved 12 July 2010 . ^ Siegelmann, H.T.; Sontag, E.D. (1991). "Turing computability with neural nets" . Appl. Math. Lett. 4 (6): 77–80. doi : 10.1016/0893-9659(91)90080-F . ^ NASA - Dryden Flight Research Center - News Room: News Releases: NASA NEURAL NETWORK PROJECT PASSES MILESTONE . Nasa.gov. Retrieved on 2013-11-20. ^ Roger Bridgman's defence of neural networks ^ http://www.iro.umontreal.ca/~lisa/publications2/index.php/publications/show/4 Bibliography [ edit ] Bhadeshia H. K. D. H. (1999). "Neural Networks in Materials Science" . ISIJ International 39 (10): 966–979. doi : 10.2355/isijinternational.39.966 . Bishop, C.M. (1995) Neural Networks for Pattern Recognition , Oxford: Oxford University Press. ISBN 0-19-853849-9 (hardback) or ISBN 0-19-853864-2 (paperback) Cybenko, G.V. (1989). Approximation by Superpositions of a Sigmoidal function, Mathematics of Control, Signals, and Systems , Vol. 2 pp.303–314. electronic version Duda, R.O., Hart, P.E., Stork, D.G. (2001) Pattern classification (2nd edition) , Wiley, ISBN 0-471-05669-3 Egmont-Petersen, M., de Ridder, D., Handels, H. (2002). "Image processing with neural networks – a review". Pattern Recognition 35 (10): 2279–2301. doi : 10.1016/S0031-3203(01)00178-9 . Gurney, K. (1997) An Introduction to Neural Networks London: Routledge. ISBN 1-85728-673-1 (hardback) or ISBN 1-85728-503-4 (paperback) Haykin, S. (1999) Neural Networks: A Comprehensive Foundation , Prentice Hall, ISBN 0-13-273350-1 Fahlman, S, Lebiere, C (1991). The Cascade-Correlation Learning Architecture , created for National Science Foundation , Contract Number EET-8716324, and Defense Advanced Research Projects Agency (DOD), ARPA Order No. 4976 under Contract F33615-87-C-1499. electronic version Hertz, J., Palmer, R.G., Krogh. A.S. (1990) Introduction to the theory of neural computation , Perseus Books. ISBN 0-201-51560-1 Lawrence, Jeanette (1994) Introduction to Neural Networks , California Scientific Software Press. ISBN 1-883157-00-5 Masters, Timothy (1994) Signal and Image Processing with Neural Networks , John Wiley Sons, Inc. ISBN 0-471-04963-8 Ripley, Brian D . (1996) Pattern Recognition and Neural Networks , Cambridge Siegelmann, H.T. and Sontag, E.D. (1994). Analog computation via neural networks, Theoretical Computer Science , v. 131, no. 2, pp.331–360. electronic version Sergios Theodoridis, Konstantinos Koutroumbas (2009) "Pattern Recognition", 4th Edition, Academic Press, ISBN 978-1-59749-272-0 . Smith, Murray (1993) Neural Networks for Statistical Modeling , Van Nostrand Reinhold, ISBN 0-442-01310-8 Wasserman, Philip (1993) Advanced Methods in Neural Computing , Van Nostrand Reinhold, ISBN 0-442-00461-3 Computational Intelligence: A Methodological Introduction by Kruse, Borgelt, Klawonn, Moewes, Steinbrecher, Held, 2013, Springer, ISBN 9781447150121 Neuro-Fuzzy-Systeme (3rd edition) by Borgelt, Klawonn, Kruse, Nauck, 2003, Vieweg, ISBN 9783528252656 External links [ edit ] Wikibooks has a book on the topic of: Artificial Neural Networks Neural Networks on the Open Directory Project http://en.wikipedia.org/w/index.php?title=Artificial_neural_networkoldid=595510034 "     Categories : Computational statistics Neural networks Classification algorithms Computational neuroscience Knowledge representation Hidden categories: Pages containing cite templates with deprecated parameters Use dmy dates from June 2013 All articles with unsourced statements Articles with unsourced statements from August 2011 Articles with unsourced statements from March 2012 Articles needing expert attention with no reason or talk parameter Articles needing expert attention from November 2008 All articles needing expert attention Technology articles needing expert attention Miscellaneous articles needing expert attention Articles with unsourced statements from August 2012 Articles with Open Directory Project links Navigation menu Personal tools Create account Log in Namespaces Article Talk Variants Views Read Edit View history Actions Search Navigation Main page Contents Featured content Current events Random article Donate to Wikipedia Wikimedia Shop Interaction Help About Wikipedia Community portal Recent changes Contact page Tools What links here Related changes Upload file Special pages Permanent link Page information Data item Cite this page Print/export Create a book Download as PDF Printable version Languages العربية Azərbaycanca Bosanski Català Deutsch Español Esperanto فارسی Français Gaeilge 한국어 Հայերեն हिन्दी Hrvatski Bahasa Indonesia Íslenska עברית ქართული Latina Lietuvių Македонски Malagasy മലയാളം 日本語 Português Română Русский தமிழ் ไทย Українська اردو Tiếng Việt 中文 Edit links This page was last modified on 14 February 2014 at 22:25. Text is available under the Creative Commons Attribution-ShareAlike License Terms of Use and Privacy Policy. Wikimedia Foundation, Inc. , a non-profit organization. Privacy policy About Wikipedia Disclaimers Contact Wikipedia Developers Mobile view 
 
 The David E. Rumelhart Prize For Contributions to the Theoretical Foundations of Human Cognition Home About Rumelhart Recipients Prize Selection The Medal Symposia Home The David E. Rumelhart Prize is awarded annually to an individual or collaborative team making a significant contemporary contribution to the theoretical foundations of human cognition. Contributions may be formal in nature: mathematical modeling of human cognitive processes, formal analysis of language and other products of human cognitive activity, and computational analyses of human cognition using symbolic or non-symbolic frameworks all fall within the scope of the award. The David E. Rumelhart Prize is funded by the Robert J. Glushko and Pamela Samuelson Foundation. Robert J. Glushko received a Ph.D. in Cognitive Psychology from the University of California, San Diego in 1979 under Rumelharts supervision. He is an Adjunct Full Professor at the School of Information (I-School) at the University of California, Berkeley. The prize consists of a hand-crafted, custom bronze medal , a certificate, a citation of the awardees contribution, and a monetary award of $100,000. The 2014 David E. Rumelhart Prize Recipient The recipient of the fourteenth David E. Rumelhart Prize is Ray Jackendoff , one of the world’s leading figures in the cognitive science of language. He has developed a theory of language that articulates the contribution of each level of linguistic representation and their interaction, while also elucidating how language relates to other cognitive systems. Dr. Jackendoff is Seth Merrin Professor of Philosophy and Co-Director of the Center for Cognitive Studies at Tufts University. After receiving his BA in Mathematics from Swarthmore College in 1965, he completed his PhD in Linguistics from MIT in 1969, under the supervision of Noam Chomsky. He then joined the faculty at Brandeis University, where he remained until 2005, until taking up his current position at Tufts. Jackendoff is a Fellow of the American Academy of Arts and Sciences, of the American Association for the Advancement of Science, of the Linguistic Society of America, and of the Cognitive Science Society. David E. Rumelhart Prize Recipients This work is licensed under a Creative Commons Attribution-NonCommercial 3.0 Unported License 
 
 David MacKay Information Theory, Pattern Recognition and Neural Networks Prerequisites Summary Videos Slides 2012 Supervisions The Book Errors Software Any questions? Information Theory, Inference, and Learning Algorithms Order your copy Price: 35.00 / $60.00 from | CUP UK / USA | | amazon.co.uk / .com / .ca / .co.jp | | Barnes Noble USA | booksprice . | fetchbook.info | allbookstores | biggerbooks | blackwells | directtextbook | kalahari.net (South Africa) | Special Paperback edition for South Asia .| Download the book too and search the book on The book in one file U.K. Canada South Africa PDF (A4) pdf pdf pdf Postscript (A4) postscript postscript postscript DJVU djvu file djvu file djvu file ( djvu information | Download djView) Just the words (latex) latex latex Just the figures NEW All in one file (postscript) [provided for use of teachers] (2M) (pdf) (5M) ps.gz pdf In individual eps files postscript and pdf available from this page mirror mirror Notes Copyright issues: will remain viewable History: Here is my method for converting to two-up under linux: David J.C. MacKay 
 HOME PRODUCTS SOLUTIONS By Business Domain Active Traders Brokers Energy Retail Healthcare Insurance Science By Solution NeuroIntelligence Forecaster XL NeuroFusion Tradecision Forecaster NeuroDienst BrokerBOOSTER iBrokerAge Customers Testimonials SERVICES Overview Outsourced Product Development Expertise Experience Business Models Product Delivery PURCHASE SUPPORT Support Centre Priority Annual Support Support Request Request a Feature COMPANY Company Info Mission Consulting Technology Research Contact Us Highlights Forecasting Excel add-in Credit Scoring Software Solutions by Industry Brokerage Energy Healthcare Insurance Retail Science About Alyuda Research More about Alyuda. Neural Network Software For researchers, data mining experts and predictive analysts Featured products Neural networks software Alyuda NeuroIntelligence Forecasting Software business analysts and managers Alyuda forecasting software Featured solutions Forecasting Excel add-in Alyuda Forecaster XL Trading Software For active stock, forex and futures traders trading software Featured product Technical analysis software and neural network trading software Alyuda Tradecision Credit Scoring System For risk managers, retail credit analysts and scorecard modelers credit scoring systems Featured partner product Credit scoring system - PlugScore . All rights reserved. Terms of Use Privacy Policy Contact us Site map Ask Us a Question Ask Us a Question *Indicates a required field Name*: E-mail*: Company*: Please enter your message here*: Request a Quote Request a Quote *Indicates a required field Name*: E-mail*: Company*: Select the item you are interested in: -- By Business Domain -- Active Traders Brokers Financial Institutions Energy Healthcare Insurance Science -- By Solution -- BrokerBooster NeuroIntelligence NeuroDienst Tradecision PlugScore Forecaster XL Forecaster NeuroFusion Please enter your message here*: 
 Contact NeuroDimension Home Products NeuroSolutions TradingSolutions Trader68 Neural Network Course OptiGen Library Interactive Book Services Neural Network Consulting Financial System Development Custom Software Applications Main Page Business Finance & Trading Medical Science Sports Resources Intro to Neural Networks Intro to Genetic Algorithms Customer Interviews Customer List Company History Support Contact Us Order Secure Online Order Order Information Product Advisor Product Pricing Level Summary Licensed User Center Neural Networks and Intelligent Software Solutions Neural networks Perform cluster analysis, sales forecasting, sports predictions, medical classification, and much more with NeuroSolutions , including versions for Excel and MATLAB . Perform price prediction and advanced stock, FOREX and Futures trading analysis with TradingSolutions and automatically trade real-time signals with Trader68 . Utilize advanced optimization techniques including Genetic Optimization and Attribute Searching in NeuroSolutions , powered by our OptiGen Library . Leverage over 20 years of industry experience with our advanced consulting services , Neural Network Courses , and Interactive Book . Harness the massive processing power of NVIDIA TM CUDA TM for speed improvements of up to hundreds of times faster in NeuroSolutions ! Find out more about Neural Networks and Neural Network Applications or visit the Product Advisor to determine which products best meet your needs. New Product: NeuroSolutions Accelerator add-on for supercomputer-like speeds! Click here to find out more! New NeuroSolutions for MATLAB 4 now supporting R2013a, 64-bit and more! Check out What's New in NeuroSolutions 6.31 for complete details on the latest release of NeuroSolutions. New NeuroSolutions Tip Box: Click here to find out more! NeuroSolutions neural network software is the perfect tool for solving your data modeling problems. Download a FREE evaluation Consulting Services consulting services TradingSolutions trading software that combines neural network and genetic algorithm technologies with traditional technical analysis to create a highly effective tool for financial modeling. Download a FREE evaluation copy Trader68 auto-trading software OptiGen Library OptiGen Library Neural Network Course Neural Network Course " NeuroDimension, Inc. All trademarks and copyrights are the property of their respective owners. Privacy Policy Become an Authorized Reseller ND.com NeuroSolutions.com Trader68.com TradingSolutions.com " 
 Home Markets Products Clients Services Contact Us Understand, Predict and Optimize Your Operations... with Unheard of Returns on Investment. Solving Tough Customer Challenges Advanced Technologies Customer Resources Profit / Dakota Forums Preferred Customer Program                  Order Products and Updates Sign-Up for Our Financial Newsletter ... or Our Commercial Newsletter Financial (Dakota/Profit) Commercial/Industrial IntelliDynamics Intellect 3.0 Intellect 3.0 is the next generation of real-time performance enhancement and predictive intelligence systems. First destined for process engineering / manufacturing applications, Intellect 3.0 will help producers increase their performance during these challenging economic times. With its history of more than doubling yields and manufacturing capacities, Intellect has proven itself an excellent return on investment in all manufacturing market segments. learn more Customer Assistance We have a new Customer Assistance site. Check it out... Help Desk Privacy Policy 
 Lester Ingber's Archive Subscribe Lester.Ingber.com Lester Ingber, Ph.D. (Theoretical Physics) http://www.ingber.com/ http://alumni.caltech.edu/~ingber/ (mirror) E-mail queries may be addressed to lester@ingber.com ingber@alumni.caltech.edu Lester Ingber Research (LIR) ingber_terms.html . LIR-Payment . SSL access, accepting my Certificate, is through https://www.ingber.com Search ingber.com at ingber_search.html . Categories of research files in this / directory are AUTHOR KARATE ASA ASA-CODE , ASA-REPRINTS COMBAT MARKETS NUCLEAR PATH-INTEGRAL NEOCORTEX Additional Files and Information See utils_file_formats.txt utils_code.html netlinks.html private/ louise/ louise/louise_ballet.html http://alumni.caltech.edu/~ingber/ This is my mirror homepage containing just their own html files, Unmanaged VPS Some information on our home for sale is in ashland4sale ashland.html [ To-Top-of-Archive ] AUTHOR ingber_CV.pdf or ingber_CV.ps.gz or ingber_CV.txt Lester Ingber's Curriculum Vitae ingber_summary_1.pdf ingber_summary_1.txt Projects and interests are described in ingber_projects_brief.pdf Additional information related to some projects is in ingber_projects.html ingber_projects ingber.bib.html ingber.ref.html ingber.bib ingber.end ingber.ref ingber.ref ingber.ris ingber.xml LIR terms of use and downloading policies are discussed in ingber_terms.html Some reasonable conditions for collaboration are discussed in lir_computational_physics_group.html http://www.xsede.org You can view my Profile on LinkedIn http://www.linkedin.com/in/ingber Google+ Profile at http://google.com/+LesterIngber/about You can view my Facebook Profile at http://www.facebook.com/lester.ingber/about ingber82_LegendsOfCaltech.pdf %A L. Ingber %T The OXY cornerstone %B Legends of Caltech %E W.A. Dodge, Jr. %E R.B. Moulton %E H.W. Sigworth %E A.C. Smith, Jr. %I Alumni Association, California Institute of Technology %C Pasadena, CA %D 1982 %P 28 ingber12_EandS.pdf %A Anonymous %T Endnotes %J (Caltech) Engineering and Science %V 75 %D 2012 %P 44 [ To-Top-of-Archive ] KARATE Lester Ingber, 8th Dan karate.html karate.txt karate72_encinitas.jpg Lester Ingber photo Encinitas, CA 30 Mar 1972 karate72_learning.pdf %A L. Ingber %T Editorial: Learning to learn %J Explore %V 7 %P 5-8 %D 1972 karate76_book.html %A L. Ingber %T The Karate Instructor's Handbook %I Physical Studies Institute-Institute for the Study of Attention %C Solana Beach, CA %D 1976 http://www.ingber.com/karate76_book.txt karate76_cover.gif karate76_cover_back.gif karate81_book.txt %A L. Ingber %T Karate: Kinematics and Dynamics %I Unique %C Hollywood, CA %D 1981 karate81_cover.gif karate81_attention.pdf %A L. Ingber %T Attention, physics and teaching %J Journal Social Biological Structures %V 4 %P 225-235 %D 1981 %O URL http://www.ingber.com/smni81_attention.pdf smni72_learning.pdf 1982 Training Videos karate85_book.html These videos may be viewed as full training sessions from my Drive archive karate82_exam karate82_Earth karate82_Wind karate82_Fire karate82_Water karate82_Void karate82_exam.mpg karate82_Earth.mpg karate82_Wind.mpg karate82_Fire.mpg karate82_Water.mpg karate82_Void.mpg http://www.youtube.com/user/ingber#p/p karate85_book.html %A L. Ingber %T Elements of Advanced Karate %I Ohara %C Burbank, CA %D 1985 http://www.ingber.com/karate85_book.txt karate85_cover.pdf karate85_cover.ps.gz karate85_cover.gif karate85_cover.txt karate85_photo.jpg karate00_keri_no_kata.html %A L. Ingber %T Keri No Kata %J Shotokan Research Society International (SRSI) %V 1 %N 4 %D 2000 [ To-Top-of-Archive ] ASA ASA-CODE Adaptive Simulated Annealing (ASA) %A L. Ingber %T Adaptive Simulated Annealing (ASA) %R Global optimization C-code %I Caltech Alumni Association %C Pasadena, CA %D 1993 Mirrors of the ASA code are at ASA-README.html ASA-README.pdf or ASA-README.ps.gz ASA-README.txt ASA.tar.gz ASA.zip asa_new.txt asa_papers.html asa_retrieve.txt asa_contrib.txt asa_examples.txt [ To-Top-of-Archive ] ASA-REPRINTS asa89_vfsr.pdf or asa89_vfsr.ps.gz %A L. Ingber %T Very fast simulated re-annealing %J Mathematical Computer Modelling %V 12 %N 8 %P 967-973 %D 1989 asa92_mnn.pdf or asa92_mnn.ps.gz %A L. Ingber %J Physical Review A %V 45 %N 4 %P R2183-R2186 %D 1992 asa92_saga.pdf or asa92_saga.ps.gz %A L. Ingber %A B. Rosen %J Mathematical Computer Modelling %V 16 %N 11 %P 87-100 %D 1992 asa93_sapvt.pdf or asa93_sapvt.ps.gz %A L. Ingber %T Simulated annealing: Practice versus theory %J Mathematical Computer Modelling %V 18 %N 11 %D 1993 %P 29-57 asa96_lessons.pdf asa96_lessons.ps.gz %A L. Ingber %T Adaptive simulated annealing (ASA): Lessons learned %J Control and Cybernetics %V 25 %N 1 %P 33-54 %D 1996 asa96_vidal_nahorski.txt asa01_lecture.html %A L. Ingber %R ASA-PATHINT Lecture Plates %I Lester Ingber Research %D 2001 asa01_lecture.pdf asa01_lecture.ps.gz asa03_reinforce.pdf %A A.F. Atiya %A A.G. Parlos %A L. Ingber %I IEEE CAS %C Cairo, Egypt %D 2003 asa06_ism.pdf %A L. Ingber %T Ideas by statistical mechanics (ISM) %R Report 2006:ISM %I Lester Ingber Research %D 2006 asa11_options.pdf %A L. Ingber %T Adaptive Simulated Annealing %E H.A. Oliveira, Jr. %E A. Petraglia %E L. Ingber %E M.A.S. Machado %E M.R. Petraglia %I Springer %C New York %D 2012 %P 33-61 [ To-Top-of-Archive ] COMBAT combat85_smart.pdf combat85_smart.ps.gz %A L. Ingber %T Statistical mechanics algorithm for response to targets (SMART) %I American Association for Artificial Intelligence %C Menlo Park, CA %D 1985 %P 258-264 combat86_approach.pdf combat86_approach.ps.gz %A L. Ingber %P 237-244 %D 1986 %I MIT %C Cambridge, MA combat91_data.pdf combat91_data.ps.gz %A L. Ingber %A H. Fujio %A M.F. Wehner %J Mathematical Computer Modelling %V 15 %N 1 %P 65-90 %D 1991 combat91_human.pdf combat91_human.ps.gz %A L. Ingber %A D.D. Sworder %T Statistical mechanics of combat with human factors %J Mathematical Computer Modelling %V 15 %N 11 %D 1991 %P 99-127 combat93_c3sci.pdf combat93_c3sci.ps.gz %A L. Ingber %T Statistical mechanics of combat and extensions %B Toward a Science of Command, Control, and Communications %E C. Jones %I American Institute of Aeronautics and Astronautics %C Washington, D.C. %D 1993 %P 117-149 combat97_cmi.pdf combat97_cmi.ps.gz %A M. Bowman %A L. Ingber %T Canonical momenta of nonlinear combat %I Society for Computer Simulation %C San Diego, CA %D 1997 combat01_lecture.html %A L. Ingber %R SMC Lecture Plates %I Lester Ingber Research %D 2001 combat01_lecture.pdf combat01_lecture.ps.gz [ To-Top-of-Archive ] MARKETS Upon a reasonable request, I may make some of my MARKETS/TRD codes available for collaboration. markets84_statmech.pdf markets84_statmech.ps.gz %A L. Ingber %J Mathematical Modelling %V 5 %N 6 %P 343-361 %D 1984 markets90_interest.pdf markets90_interest.ps.gz %A L. Ingber %J Physical Review A %V 42 %N 12 %D 1990 %P 7057-7064 markets91_interest.pdf markets91_interest.ps.gz %A L. Ingber %A M.F. Wehner %A G.M. Jabbour %A T.M. Barnhill %J Mathematical Computer Modelling %V 15 %N 11 %D 1991 %P 77-98 markets96_brief.pdf markets96_brief.ps.gz %A L. Ingber %R Report 1996:TMCMASA %I Lester Ingber Research %C McLean, VA %D 1996 markets96_momenta.pdf markets96_momenta.ps.gz %A L. Ingber %B Progress in Neural Information Processing %E S.-I. Amari, L. Xu, I. King, and K.-S. Leung %I Springer %C New York %P 777-784 %D 1996 Tables of data supporting this paper are given in markets96_momenta_tbl.txt.gz markets96_lag_cmi.c ingber_projects.html markets96_trading.pdf markets96_trading.ps.gz %A L. Ingber %J Mathematical Computer Modelling %V 23 %N 7 %P 101-121 %D 1996 markets98_smfm_appl.pdf markets98_smfm_appl.ps.gz %A L. Ingber %T Some Applications of Statistical Mechanics of Financial Markets %R LIR-98-1-SASMFM %I Lester Ingber Research %C Chicago, IL %D 1998 markets98_lecture.pdf markets98_lecture.ps.gz %A L. Ingber %T Statistical mechanics of financial markets (SMFM) %R SMFM Lecture Plates %I Lester Ingber Research %C Chicago, IL %D 1998 markets99_spread.pdf markets99_spread.ps.gz %A L. Ingber %T A simple options training model %J Mathematical Computer Modelling %V 30 %P 167-182 %D 1999 markets99_vol.pdf markets99_vol.ps.gz %A L. Ingber %A J.K. Wilson %T Volatility of volatility of financial markets %J Mathematical Computer Modelling %V 29 %P 39-57 %D 1999 markets00_exp.pdf markets00_exp.ps.gz %A L. Ingber %A J.K. Wilson %J Mathematical Computer Modelling %V 31 %N 8/9 %P 167-192 %D 2000 markets00_highres.pdf markets00_highres.ps.gz %A L. Ingber %T High-resolution path-integral development of financial options %J Physica A %V 283 %N 3-4 %P 529-558 %D 2000 markets01_optim_trading.pdf %A L. Ingber %A R.P. Mondescu %T Optimization of Trading Physics Models of Markets %D 2001 %V 12 %N 4 %P 776-790 %J IEEE Trans. Neural Networks markets01_pathtree.pdf %A L. Ingber %A C. Chen %A R.P. Mondescu %A D. Muzzall %A M. Renedo %T Probability tree algorithm for general diffusion processes %J Physical Review E %V 64 %N 5 %P 056702-056707 %D 2001 markets01_lecture.html %A L. Ingber %R SMFM Lecture Plates %I Lester Ingber Research %D 2001 markets01_lecture.pdf markets01_lecture.ps.gz markets02_portfolio.pdf %A L. Ingber %T Statistical mechanics of portfolios of options %D 2002 %I Lester Ingber Research %C Chicago, IL markets03_automated.pdf %A L. Ingber %A R.P. Mondescu %B Intelligent Internet-Based Information Processing Systems %E R.J. Howlett %E N.S. Ichalkaranje %E L.C. Jain %E G. Tonfoni %I World Scientific %C Singapore %D 2003 %P 305-356 markets05_trd.pdf %A L. Ingber %T Trading in Risk Dimensions (TRD) %R Report 2005:TRD %D 2005 %I Lester Ingber Research %C Ashland, OR A pdf of the TRD section of the ppt presentation is in markets10_trd_present.pdf markets06_ism.pdf %A L. Ingber %T Ideas by statistical mechanics (ISM) %R Report 2006:ISM %I Lester Ingber Research %D 2006 markets07_rops.pdf %A L. Ingber %T Real Options for Project Schedules (ROPS) %R Report 2007:ROPS %D 2007 %I Lester Ingber Research [ To-Top-of-Archive ] NUCLEAR nuclear65_nonadiabatic.pdf %A L. Ingber %T Non-adiabatic corrections to the method of stationary states %J Phys. Rev. A %V 139 %P 35-39 %D 1965 %O URL http://www.ingber.com/nuclear65_nonadiabatic.pdf This was my first publication in a physics journal. nuclear68_forces.pdf %A L. Ingber %T Nuclear Forces %J Physical Review %V 174 %P 1250-1263 %D 1968 %O URL http://www.ingber.com/nuclear68_forces.pdf nuclear83_riemann.pdf %A L. Ingber %T Riemannian corrections to velocity-dependent nuclear forces %J Physical Review C %V 28 %P 2536-2539 %D 1983 %O URL http://www.ingber.com/nuclear83_riemann.pdf nuclear84_riemann.pdf nuclear84_riemann.ps.gz %A L. Ingber %J Physical Review D %V 29 %P 1171-1174 %D 1984 nuclear86_riemann.pdf nuclear86_riemann.ps.gz %A L. Ingber %J Physical Review D %V 33 %P 3781-3784 %D 1986 [ To-Top-of-Archive ] PATH-INTEGRAL Upon a reasonable request, I may make some of my PATHINT and PATHTREE codes available for collaboration. Papers markets84_statmech.pdf , markets91_interest.pdf and smni91_eeg.pdf path86_GinsburgLandau.pdf %A L. Ingber %T Noise-induced extrema in time-dependent Ginsburg-Landau systems %J Math. Modelling %V 7 %P 525-528 %D 1986 path91_data.pdf or path91_data.ps.gz %A L. Ingber %A H. Fujio %A M.F. Wehner %J Mathematical Computer Modelling %V 15 %N 1 %P 65-90 %D 1991 path94_stm.pdf or path94_stm.ps.gz %A L. Ingber %J Physical Review E %V 49 %N 5B %D 1994 %P 4652-4664 path95_stm.pdf path95_nonl.pdf or path95_nonl.ps.gz %A L. Ingber %J Physical Review E %V 51 %N 2 %P 1616-1619 %D 1995 path95_stm.pdf or path95_stm.ps.gz %A L. Ingber %A P.L. Nunez %J Physical Review E %V 51 %N 5 %P 5074-5083 %D 1995 path96_duffing.pdf path96_duffing.ps.gz %A L. Ingber %A R. Srinivasan %A P.L. Nunez %J Mathematical Computer Modelling %V 23 %N 3 %P 43-53 %D 1996 path98_datamining.pdf path98_datamining.ps.gz %A L. Ingber %J Mathematical Computer Modelling %V 27 %N 3 %P 9-31 %D 1998 path00_exp.pdf or path00_exp.ps.gz %A L. Ingber %A J.K. Wilson %J Mathematical Computer Modelling %V 31 %N 8/9 %P 167-192 %D 2000 path00_highres.pdf path00_highres.ps.gz %A L. Ingber %T High-resolution path-integral development of financial options %J Physica A %V 283 %N 3-4 %P 529-558 %D 2000 path01_pathtree.pdf %A L. Ingber %A C. Chen %A R.P. Mondescu %A D. Muzzall %A M. Renedo %T Probability tree algorithm for general diffusion processes %J Physical Review E %V 64 %N 5 %P 056702-056707 %D 2001 path01_lecture.html %A L. Ingber %R ASA-PATHINT Lecture Plates %I Lester Ingber Research %D 2001 path01_lecture.pdf path01_lecture.ps.gz path10_multiple_scales.pdf %A L. Ingber %A P.L. Nunez %J Mathematical Biosciences %D 2010 [ To-Top-of-Archive ] NEOCORTEX Upon a reasonable request, I may make some of my SMNI codes available for collaboration. smni72_learning.pdf %A L. Ingber %T Editorial: Learning to learn %J Explore %V 7 %P 5-8 %D 1972 smni81_attention.pdf %A L. Ingber %T Attention, physics and teaching %J Journal Social Biological Structures %V 4 %P 225-235 %D 1981 %O URL http://www.ingber.com/smni81_attention.pdf smni72_learning.pdf smni81_unified.pdf %A L. Ingber %T Towards a unified brain theory %J Journal Social Biological Structures %V 4 %P 211-224 %D 1981 %O URL http://www.ingber.com/smni81_unified.pdf smni82_reader.pdf Lester Ingber Prediction of neural implants The Reader Del Mar, CA 8 Jan 1982 smni82_basic.pdf smni82_basic.ps.gz %A L. Ingber %J Physica D %V 5 %P 83-107 %D 1982 smni83_dynamics.pdf smni83_dynamics.ps.gz %A L. Ingber %J Physical Review A %V 28 %P 395-416 %D 1983 smni84_stm.pdf or smni84_stm.ps.gz %A L. Ingber %J Physical Review A %V 29 %P 3346-3358 %D 1984 smni85_eeg.pdf or smni85_eeg.ps.gz %A L. Ingber %J IEEE Transactions Biomedical Engineering %V 32 %P 91-94 %D 1985 smni85_stm.pdf or smni85_stm.ps.gz %A L. Ingber %J Physical Review A %V 31 %P 1183-1186 %D 1985 smni91_eeg.pdf or smni91_eeg.ps.gz %A L. Ingber %J Physical Review A %N 6 %V 44 %P 4017-4060 %D 1991 smni92_mnn.pdf or smni92_mnn.ps.gz %A L. Ingber %J Physical Review A %V 45 %N 4 %P R2183-R2186 %D 1992 smni94_stm.pdf or smni94_stm.ps.gz %A L. Ingber %J Physical Review E %V 49 %N 5B %D 1994 %P 4652-4664 Higher resolution calculations performed with a supercomputer are in smni95_stm.pdf smni95_images.pdf smni95_images.ps.gz %A L. Ingber %T Multiple scales of brain-mind interaction %J Behavioral and Brain Sciences %V 18 %N 2 %P 360-362 %D 1995 smni95_scales.pdf smni95_scales.ps.gz %A L. Ingber %B Neocortical Dynamics and Human EEG Rhythms %E P.L. Nunez %I Oxford University Press %C New York, NY %P 628-681 %D 1995 smni95_stm.pdf or smni95_stm.ps.gz %A L. Ingber %A P.L. Nunez %J Physical Review E %V 51 %N 5 %P 5074-5083 %D 1995 smni95_stm40hz.pdf smni95_stm40hz.ps.gz %A L. Ingber %J Physical Review E %V 52 %N 4 %D 1995 %P 4561-4563 smni96_eeg.pdf or smni96_eeg.ps.gz %A L. Ingber %E R.M. Dasheiff and D.J. Vincent %I Elsevier %C Amsterdam %D 1996 %P 79-112 smni96_dasheiff_vincent.txt smni96_nonlinear.pdf smni96_nonlinear.ps.gz %A L. Ingber %J Behavioral and Brain Sciences %V 19 %N 2 %P 300-301 %D 1996 smni97_cmi.pdf or smni97_cmi.ps.gz %A L. Ingber %J Physical Review E %V 55 %N 4 %P 4578-4593 %D 1997 smni97_eeg_data.html smni97_eeg_cmi.tar.gz smni97_lecture.pdf smni97_lecture.ps.gz %A L. Ingber %T Statistical mechanics of neocortical interactions (SMNI) %R SMNI Lecture Plates %I Lester Ingber Research %C Chicago, IL %D 1997 smni98_cmi_test.pdf smni98_cmi_test.ps.gz %A L. Ingber %J Mathematical Computer Modelling %V 27 %N 3 %P 33-64 %D 1998 smni97_eeg_data.html smni97_eeg_cmi.tar.gz smni99_g_factor.pdf smni99_g_factor.ps.gz %A L. Ingber %J Psycholoquy %V 10 %N 068 %D 1999 smni00_jensen.txt smni00_eeg_rt.pdf smni00_eeg_rt.ps.gz %A L. Ingber %I World Congress on Medical Physics and Biomedical Engineering %C Chicago, IL %D 2000 smni00_eeg_stm.pdf smni00_eeg_stm.ps.gz %A L. Ingber %J Behavioral and Brain Sciences %V 23 %N 3 %P 403-405 %D 2000 smni01_lecture.html %A L. Ingber %R SMNI Lecture Plates %I Lester Ingber Research %D 2001 smni01_lecture.pdf smni01_lecture.ps.gz smni06_ism.pdf %A L. Ingber %T Ideas by statistical mechanics (ISM) %R Report 2006:ISM %I Lester Ingber Research %D 2006 smni06_ppi.pdf %A L. Ingber %R Report 2006:PPI %I Lester Ingber Research %D 2006 smni07_timedelays.pdf %A L. Ingber %T Statistical mechanics of neocortical interactions: Time delays %R Report 2007:TD %I Lester Ingber Research %D 2007 smni08_tt.pdf %A L. Ingber %J NeuroQuantology Journal %V 6 %N 2 %D 2008 %P 97-104 smni09_columnar_eeg.pdf %A L. Ingber %T Statistical mechanics of neocortical interactions: Columnar EEG %R Report 2009:CEEG %I Lester Ingber Research %D 2009 smni09_nonlin_column_eeg.pdf smni10_multiple_scales.pdf smni09_nonlin_column_eeg.pdf %A L. Ingber %J NeuroQuantology Journal %V 7 %N 4 %P 500-529 %D 2009 The next study in smni10_multiple_scales.pdf smni10_multiple_scales.pdf %A L. Ingber %A P.L. Nunez %J Mathematical Biosciences %D 2010 smni11_cog_comp.pdf %A L. Ingber %E A. Pereira, Jr. %E E. Massad %E N. Bobbitt %I Springer %C New York %D 2011 smni11_stm_scales.pdf %A L. Ingber %B Short-Term Memory: New Research %E G. Kalivas %E S.F. Petralia %D 2012 %P 37-72 %I Nova %C Hauppauge, NY %O Invited Paper. URL http://www.ingber.com/smni11_stm_scales.pdf smni12_vectpot.pdf %A L. Ingber %T Influence of macrocolumnar EEG on Ca waves %J Current Progress Journal %D 2012 %V 1 %N 1 %P 4-8 %D 2012 smni13_eeg_ca_lect.pptx %A L. Ingber %R Report 2013:LFIC %T Electroencephalographic (EEG) influence on Ca 2+ waves %D 2013 %I Lester Ingber Research %C Ashland, OR smni14_eeg_ca.pdf %A L. Ingber %A M. Pappalepore %A R.R. Stesiak %J Journal of Theoretical Biology %D 2014 smni14_eeg_ca_supp.pdf smni14_eeg_ca_audio-slides.mp4 http://url.ingber.com/eeg_ca smni14_eeg_ca_slides.pdf smni14_ismai.pdf %A L. Ingber %T Ideas by multiple scales of statistical mechanics (ISM) %J International Journal of Robotics Applications and Technologies (IJRAT) %D 2014 smni14_conscious_scales.pdf %A L. Ingber %T Influences on consciousness from multiple scales of neocortical interactions %B Non-chemical distant cellular interactions: experimental evidences, theoretical modeling, and physiological significance %E M. Cifra %E V. Salari %E D. Fels %E F. Scholkmann %D 2014 %O Invited Paper. URL http://www.ingber.com/smni14_conscious_scales.pdf [ To-Top-of-Archive ] $Id: index.html,v 11.726 2014/02/24 16:51:54 ingber Exp ingber $ 
 
 About Attrasoft 1. Find an image in a billion images in 3 seconds. 2. Find a video in 1 million videos in 3 seconds. 3. Tag images automatically using software. Download image demo, ImageFinderLite. Attrasoft, Inc. is an Image-Recognition Search Engine company with market proven core technology. Market Pain 95% of Digital Universe is not searchable via keywords (IDC white paper). Number of Net images will double in three years and redouble in another three years, thus exacerbating the problem. Manual tagging is labor intensive, time consuming, and unscalable. Yields millions of results. Difficult to search via keyword. All possible Internet locations not provided. Inaccurately tagged, or mislabeled (human error versus sometimes intentionally). Service Attrasoft specializes in image tagging using computer software. ImageTagger Product Mini-AttraSeek is our flagship product: It provides automated image matching within your website or database without manual tagging . It is an off-the-shelf product. It can be deployed in your website or hosted/managed by Attrasoft for your website. Applications To search untagged images, which are 95% of all images; To check newly uploaded images, which have no tags; To aid commerce sites where people perform web search for product purchase and search management; To search within your website or database for images where image tagging is difficult or requires expertise; To increase sales for companies selling over the web where image is critical to the sale, examples includes art, photos, products with visual similarities such as wheel rims, real estate, dinner plates, etc. To create cell phone apps that require image recognition. To reduce manual labor in reviewing surveillance video. Benefits Reduces your manual image compliance search personnel by 90%. Eliminates the manual image tagging process by 90% or more. Reduces user search time from 5 minutes to 5 - 20 seconds , which can increase sales for companies selling over the web where image is critical to the sale (i.e., art, wheel rims, real estate, home decor items, etc.). Increase searchable images by a factor of 20 (From 5% searchable images to 100% searchable images). Provides for a wide range of applications for any web-based image location need. Gives confidence scores to aid user discernment of exact and similar matches. Reduces the time required by the user to do the same image search with word descriptions by 50 % or more. Uses patent-pending algorithms to compare images to images without text descriptions. Solves image tagging problems such as inaccurate tag, mislabeled tag, lacking proper details, and other human errors Testimonials Dramatic Difference Demos The demos below allow anyone to test the ease-and-use of the technology by accurately locating ANY image among a large database of images in seconds. One Quick Test with Google Logos. Many Quick Tests with Google Logos. Many Quick Tests with Product Images. Many Quick Tests with Art images. Many Quick Tests with Product Images. Many Quick Tests with multiple scanned advertisement images. Many Quick Tests with multiple scanned document images. Do your own test with 40,000 test images online. Local testing with your own test images. Video search demo. What is New? IFSurveillance 2013 (Feb 2013 ) ImageFinderSeg 2013 (Jan 2012) TransApplet ImageFinder FlashFinderLite VideoFinderLite ImageFinderLite Image Tagging Service Features Is scalable to billions of images. Has constant search time regardless of the number of images in the searched database. Is 100% accurate if the image is in the database. Can accommodate small variations. Can accommodate larger variations with multiple image signatures. Guarantee Price Contact gina@attrasoft.com Home Company Services Products Customers Contact Us 
 Just Reworking the Site... Well, the last time I overhauled the hav.com website was back in 1998 - I had to verify that, in a fun way, using the Internet WayBack Machine . Soooo... some things may go away , temporarily or maybe forever. But, if you miss something in particular, let me know fun pages Anyway - enjoy the new site and watch out for falling rocks... Kicker [ HOME ] havBpNet:J , havFmNet:J , havBpNet++ , havFmNet++ , havBpETT , havCNet , WebSnarfer , havIndex and havChat are all trademarks of hav.Software hav.Software 6,276,052 / 29,178,545 Page Modified Mon Sep 19 16:28:39 CDT 2011 Open source, ... Just for Fun ... Copyright 1994-2014 by hav.Software and Horace "Kicker" Vallas. All Rights Reserved. Products Demos About havChat havBpNet++ havBpNet:J havFmNet++ havFmNet:J havETT Java Javascript JSP/Servlets Tclset LiveDemo BerkeleyDBDemo JSPSnoop NeoWebScript GDGraphics SimpleChart AveragedChart Gauge CustomizableIcons GDFonts WeakBlur HomegrownImagemap Guestbook Staticclock Who is hav.Software Services Clients Contact info Compliance Validation 
 BrainCel Neural Net Module for Microsoft Excel AmiBroker eSignal Generic DLL MATLAB MetaStock MultiCharts NeoTicker NeuroShell NinjaTrader Tradecision TradeStation leading indicators. Futures Aug '94 Futures May '96 Futures Dec '96 Microsoft Excel TradeStation 2000 i , 6, 7, 8 i , 6, 7 or 8... in our Free Stuff section. Microsoft EXCEL 97, 2000, 2002 Microsoft EXCEL 2003 Microsoft EXCEL TradeStation and select the department. TECHNICAL REPORTS. CUSTOMER's LETTERS. Read our TECHNICAL REPORTS page. [ Copyright ] [ WebMaster ] 

 About AUAI Conference Proceedings view online about UAI Mailing List subscribe People Wiki The AUAI Board Upcoming Conference UAI 2014 Conference Web Archive UAI-2013 (Bellevue) UAI-2012 (Catalina Isl.) UAI-2011 (Barcelona) UAI-2010 (Catalina Isl.) UAI-2009 (Montreal) UAI-2008 (Helsinki) UAI-2007 (Vancouver) UAI-2006 (Cambridge) UAI-2005 (Edinburgh) UAI-2004 (Banff) more... About AUAI Conference on Uncertainty in Artificial Intelligence UAI-2014 in Quebec City, Quebec, Canada, July 23 to 27th 2014. Join our Facebook group or add yourself to the UAI Mailing list to keep updated on announcements and relevant AI news. Accessing Proceedings Brightdoc . . The UAI Mailing List To Subscribe Fill out this form. UAI mailing list czar . The UAI Wiki Visit the Previous UAI Conferences Conf # Year Location 29 2013 Bellevue, Washington, USA 28 2012 Catalina Island, California, USA 27 2011 Barcelona, Spain 26 2010 Catalina Island, California, USA 25 2009 Montreal, Canada 24 2008 Helsinki, Finland 23 2007 Vancouver, Canada 22 2006 Cambridge, Massachusetts, USA 21 2005 Edinburgh, Scotland, UK 20 2004 Banff, Canada 19 2003 Acapulco, Mexico 18 2002 Edmonton, Canada 17 2001 Seattle, Washington, USA 16 2000 Stanford, California, USA 15 1999 Stockholm, Sweden 14 1998 Madison, Wisconsin, USA 13 1997 Providence, Rhode Island, USA 12 1996 Portland, Oregon, USA 11 1995 Montreal, Canada 10 1994 Seattle, Washington, USA 9 1993 Washington, DC, USA 8 1992 Stanford, California, USA 7 1991 Los Angeles, California, USA 6 1990 Cambridge, Massachusetts, USA 5 1989 Windsor, Ontario, USA 4 1988 Minneapolis, Minnesota, USA 3 1987 Seattle, Washington, USA 2 1986 Philadelphia, Pennsylvania, USA 1 1985 Los Angeles, California, USA The AUAI Board Directors of AUAI Kathryn Blackmond Laskey , David Poole , Prakash P. Shenoy , Former Directors , Chair (Microsoft Research) (Oregon State University and Cleverset) Finn Jensen Michael Wellman Tod Levitt (C4I Center, George Mason University) John Lemmer Peter Cheeseman Webmaster Mark Crowley , email here 
 Qualitative Verbal Explanations in Bayesian Belief Networks Author: Marek J. Druzdzel University of Pittsburgh Intelligent Systems Program marek@sis.pitt.edu Abstract: Keywords: PostScript PDF Back to list of publications Back to Marek's home page marek@sis.pitt.edu 
 [Next] [Up] [Previous] Next: Introduction Adnan Darwiche Gregory Provan Abstract: generation Introduction Query DAGs Implementing a Q-DAG Evaluator The Availability of Evidence Generating Query DAGs The Clustering Algorithm Generating Q-DAGs An Example Computational Complexity of Q-DAG Generation Other Generation Algorithms Soundness of the Q-DAG Clustering Algorithm Reducing Query DAGs Reductions Network Pruning Computation Caching Optimization in Belief-Network Inference A Diagnosis Example Q-DAG Generation Q-DAG Reduction Q-DAG Evaluation Concluding Remarks Proof of Theorem References About this document ... 
 
 a briefing document Custom Search Cause, chance and Bayesian statistics is one in a series of documents showing how to apply empiric reasoning to social and psychological problems. . Intelligence: misuse and abuse of statistics establishment psycho-bunk cause, chance and Bayesian statistics For related empiric reasoning documents, start with Index Introduction Black and blue taxis Testing for rare conditions How bad can it get? Endnotes Introduction Bayes [1] . or [2] in specifying a prior distribution [3] [4] ). Consider a commonly cited scenario. First piece of data: Second piece of data: misidentify For the 85 black taxis, he would also incorrectly . The false negatives among many others th advertising advertising The problem of false positives Among 1,000 tested for the disease and who do not Consider a concrete example [5] In general: Some reference keywords/tags: Related further reading establishment psycho-bunk cause, chance and Bayesian statistics Endnotes Enc Brit. (with much modification). An example where notional and or Go to email abelard at abelard.org abelard, 2002, 13 october all rights reserved the address for this document is http://www.abelard.org/briefings/bayes.htm 2200 words prints as 4 A4 pages (on my printer and set-up) latest abstracts briefings information headlines resources interesting about abelard 
 NIPS 2001 Tutorial: Nir Friedman and Daphne Koller Powerpoint Presentation Show (3.25MB) Online Presentation Printout, 6 slids per page, no animation Postscript (7.12MB) Compressed Postscript (1.66MB) PDF (1.72MB) Bibliography and Additional Readings Postscript PDF nir@cs.huji.ac.il 
 Username: Password: New user? Forgotten your password? To post or edit items: 1. Log in. 2. Go to the page to be modified 3. Choose the option to post or edit Home Publications People Conferences Software Tutorials Other Newsletter Site created by and the . Contact us@beliefrevision.org Page load time: 0.0s 
 Home People Projects Publications Misc DAGS -- Professor Daphne Koller's More info. DAGS 
 Contents Introduction Background Research Overview Probabilistic Modelling Image Understanding Astro-informatics Medical Imaging Publications Contact Viasul iulsloin of the the mnoth Hough Transform Amos Storkey Tutorial: Introduction to Belief Networks As an introduction to the research I have been doing using and developing belief network approaches, I thought it might be useful to provide a basic introduction to what I am talking about. If you find this tutorial useful then please put a link to it on your home page. To switch to a printable form of this document hit here . Reload to return to the original form. Belief Networks and Probabilistic Graphical Models Belief networks (Bayes Nets, Bayesian Networks) are a vital tool in probabilistic modelling and Bayesian methods. They are one class of probabilistic graphical model. In other words they are a marriage between two important fields: probability theory and graph theory. It is this combination which makes them a powerful methodology within machine learning and statistics. Use of belief networks has become widespread partly because of their intuitive appeal. A given belief network is very easily related to a particular probabilistic model, and is easily understood in turns of direct dependence. Because they are useful in simplifying the manipulations in Bayesian inference they have a great practical use in real world problems. Belief networks, or Bayes Nets as they are sometimes known, have been used widely from medical diagnosis to image modelling, from genetics to speech recognition, from economics to space exploration. They can be valuable in any area where there is the requirement for finding out about unknown variables through the utilisation of structural relationships and data. Even if the exact form of the relationships between the variables of interest are not known it does not matter, because the uncertainty can be represented probabilistically. Introduction to Bayesian Methods Although belief networks are a tool of probability theory, their most common use is within the framework of Bayesian analysis. This is one reason they are often called Bayesian networks (more importantly Bayes theorem is at least implicitly used in calculations with Bayes nets). The Bayesian approach is based on three fundamental beliefs: In order to infer anything from data, we must have and use prior information. Most problems involve some levels of uncertainty. Probability theory is the best (or a good) way of representing uncertainty because of Bayes theorem. The Bayesian approach to problems can be summed up in this simple way: Suppose we have some sets of variables of interest, Q and D . We use prior knowledge to define a probability distribution P(Q,D) over the variables of interest. We then receive some data telling us what values D takes. We update our belief about Q by calculating P(Q|D) , the probability of Q given we know what D is. The big issues in Bayesian methods involve how to find out what form P(Q,D) should take, and how to calculate P(Q|D) . Belief networks help with both of these problems. Belief Networks A belief network is a directed graph. That is, it consists of vertices (or nodes) and directed edges (arrows). Each edge points from one node (called the parent) to another node (known as the child). In a belief network each node is used to represent a random variable, and each directed edge represents an immediate dependence or direct influence. Although not strictly correct, one of the easiest ways to start thinking about belief networks is in term of causal (but not deterministic) dependence. For example we might be interested in diagnosing whether the household dog has fleas! There are some obvious variables which we would want to include in modelling this scenario: how much the dog is scratching (observable), whether the dog is molting (hidden), whether there are an excess of dog hairs around the house (observable), whether the cat has recently had fleas (observable) etc. We can build a belief network of this scenario by thinking in terms of causal relationships. We use these causal relationships to specify the direct dependencies in the graphical model. For example, if the dog is molting then this would probably cause some increase in the amount of scratching, and also an increase in the number of dog hairs around the house. Fleas would also probably cause the dog to itch more. The fact that the cat has had fleas might cause our dog to get fleas too, if they had been in contact. We can use this information to specify the form of graphical model to the right. Note that this is not the 'right' graphical model; it is merely one way of describing the dependence relationships. Indeed we have neglected some direct dependence which might (or perhaps should) have been included; for example scratching will probably increase the number of hairs about irrespective of the cause of that scratching. Also, we will see later that the same probability distribution can be specified using many different graphical models. Even so, this causal approach is a useful way of thinking about graphical models in the beginning. There can be a question as to how to choose what variables to include. Surely at the end of the day the whole world needs to be included in the belief network, at least in theory? In fact this is not the case. It is possible to marginalise, or integrate out, the effects of certain random variables. This means that the effect of the marginalised random variable is incorporated in the remaining variables and the conditional distributions which remain. On the other hand it is possible to assume other variables to be fixed - we might be only considering a limited scenario, for example we might assume we are on planet earth and therefore do not have to consider variations in the gravitational force! In theory then, given a finite belief network, one is assuming that the each part of the `rest of the universe' has been taken to be fixed or has been integrated out! Although the belief network is related to the prior probability distribution we need to build, it does not fully specify it. What it does give is the conditional dependencies in the probability distribution. Given a belief network a minimum set of conditional independencies can be found using a rule called D-separation. On top of these direct dependence relationships, we need to give some numbers (or functions) which define how the variables relate to one another. In order to see how this works it is best to look first at the relationship between a belief network and a probability distribution. A joint probability distribution P(A,B,C,D,E) over a number of variables can be written in the following form using the chain rule of probability: P(A,B,C,D,E) = P(A) P(B|A) P(C|B,A) P(D|C,B,A) P(E|D,C,B,A). - (eq. 1) Sometimes a probability distribution is not so general; that is there are a number of conditional dependencies which might arise. For example if we know A , C might no longer depend on what value B takes. Other such conditional dependencies could occur which enable use to write the probability in a simpler form - something like P(A,B,C,D,E) = P(A) P(B|A) P(C|A) P(D|B) P(E|D,B). - (eq 2) The key is that this form of probability representation directly relates to a particular belief network. This belief network can be constructed in the following way. For each element of the above product we build a node of the network. For example for P(C|A) we create a node corresponding to the random variable A . For each element of the above product, we connect the relevant node to the nodes which that variable depends on. These nodes become the parents of the relevant node. For example, for the factor P(E|D,B) we would draw a directed edge from the node D to the node E and another directed edge from the node B to the node E . Hence the whole belief network would look like figure CONSTBEL. A,B,C,D and E and reapply the chain rule. This will give a different root node and different structures. The key is to choose a representation which allows the conditional independence relationships to be utilised in the best way possible. It is worth making a note about a causal view of belief networks at this point. There are a number of ways that this view is problematic. The first is that there are many ways of representing a probability distribution as a belief network, and many of those will not relate to causal structures. Further, marginalisation over (that is summing out the effect of) a variable which two other variables causally depend on will create a dependence between those other variables. However neither one can really be considered to be causally dependent on the other. At best then a causal view of belief network construction is anaemic. The causal view becomes fundamentally wrong when the process is inverted and causal relationships are inferred from a belief network. This is one of the most serious flaws which can be made in statistics. Probabilistic relationships do not imply causality, even if the belief network was constructed using prior causal information. To deal with causality is significantly more complicated and involves the consideration, among other things, of counterfactuals. This is well beyond the scope of this tutorial, but readers interested in dealing with causality in this sort of framework should refer to [Pearl (2000) CAUSALITY: Models, Reasoning, and Inference] Now it is possible to understand what the links in the belief network mean, and hence what we need to specify to turn the belief network into a probability distribution. For each node we need the conditional probability of that node taking a certain value given the values of its parents. For discrete networks (nodes taking a fixed number of classes) this amounts to defining a conditional probability table. The remaining question is where we get these probabilities from. One possibility is that we know many of them a priori. Then we can just write the probabilities down and we have defined everything we need to. More likely, we do not know what values the probabilities should take. Then we can define a vaguer prior over the possible values the probabilities take and then learn the probabilities from data (In true Bayesian terms we will be calculating a posterior distribution for the values of the probabilities given some particular data.). This will be discussed further when the EM algorithm is introduced. Inference in Belief Networks The previous section focussed on the issue of building belief networks and specifying probabilities. In this section we will assume that this has been done, and we know what the conditional probabilities are. In other words we have fully specified the joint distribution of all the variables. In a sense we assume we have dealt with (in one particular way) the first major issue in Bayesian methods: knowing and specifying the prior distribution. The next big issue is calculating the conditional distribution when we find out what values some of our variables take. This process is called inference. P(Q|D) that the questions Q have certain answers given the data D , which defines the knowledge about the specific circumstances. Converting this to a belief network framework, we want to know the probability distribution of a certain set of nodes, given the fact that we know what values another set of nodes take. Unfortunately in many circumstances this process is hard. That is the computation needed to calculate the probabilities scales badly with the number of nodes (in general the problem is NP hard). There are some specific cases when inference is straightforward. In other cases people generally resort to approximation methods. At this stage, and before looking at more general algorithms, an illustration of belief Of course a belief network alone is not good enough, we need some numbers too. We might choose the following conditional probability tables (CPTs) as being reasonably representative of those probabilities. In real life situations we would probably want to tune or learn these CPTs based on historical examples. P(E=job) P(E=unemp) P(M=good) P(M=bad) E M P(S=spend) P(S=thrift) job good job bad unemp good unemp bad E S P(B=credit) P(B=debt) job spend job thrift unemp spend unemp thrift These tables specify all the conditional probabilities that are needed. Belief network construction is finished. This leads to the topic of this section. Inference is modifying beliefs in light of evidence. Suppose we know that a person is in debt, and has been spending a lot of money. We might want to know whether or not they have a job. We are interested in the posterior probability P(E|B=debt,S=spend) . To calculate this probability we first calculate P(E,M|B=debt,S=spend) . This is proportional to P(B|E,S)P(S|E,M)P(E)P(M) by simple application of probability theory (in fact using Bayes rule). Plugging in the numbers from each of the tables (looking at the B=debt,S=spend columns/rows), and normalising so that the probabilities sum to one to account for the constant of proportionality, we get E M P(B|E,S)P(S|E,M)P(E)P(M) P(E,M|B=debt,S=spend) job good job bad unemp good unemp bad where we have rounded to a couple of decimal places. To find P(E|B=debt,S=spend) we have to sum over all the possible values that M takes. This gives the final result P(E=job|B=debt,S=spend) P(E=unemp|B=debt,S=spend) From this table we can see that the probability of the person being unemployed goes up a little (from 0.1 to 0.12) in light of the information we have. If on the other had we had asked about their money management skills we would get P(M=good|B=debt,S=spend) P(M=bad|B=debt,S=spend) In other words we would get a more significant change (from 0.4 to 0.56) in the probability that they had poor money management skills. Hopefully this has illustrated what is meant by inference in belief networks. An example like this is particularly useless in practice because a) the number of variables and states is too small making the network oversimple for the task at hand, and b) because I plucked the numbers out of thin air! Subject a) is the topic of the next section, and b) is the subject of the following section on learning in belief networks. Exact and Approximate Inference One situation when belief network inference is straightforward is in singly connected networks. A singly connected network is a graph where there is only one undirected path from any node to any other node (in an undirected path the arrow directions are ignored). In this situation there is a simple approach to solving the inference problem which is called belief propagation. Consider a tree structured network, which is a basic sort of simply connected graphical model. If we want to condition on certain nodes of this tree structured network, it does not introduce serious problems. This is because conditioning on a node splits the network into two parts, the descendants of the conditioned node, and the remainder of the network. In each of these parts the new information at the conditioned nodes effects the nodes which it is directly connected to. These in turn effect the nodes they are connected to and so on. Because there are no loops in the network, it is possible to follow the chain of effect until all the nodes are reached once and only once. It turns out that there is a simple way to propagate beliefs through these networks in order to obtain the updated probabilities. This is called belief propagation due to Pearl [1988]. It involves passing a set of messages up the tree from the conditioned nodes (called the lambda messages, and analogous to likelihoods), and then a further set of messages down the tree (called pi messages and analogous to priors). The final posterior marginal probabilities at the nodes are proportional to the product of the lambda and pi messages at each of the nodes. (In fact the whole posterior distribution can be reconstructed from this information as the belief network of the posterior is a set of disjoint trees). I will not duplicate the belief propagation algorithm and theory here, as it is available in many places. Learning and the EM algorithm The final issue which will be considered here involves the question about how to set or learn the conditional probabilities in our belief network. In practice these are often learnt from past data. In fact learning the conditional probability parameters of a belief network is another example of Bayesian inference, just inference at a higher level: we are inferring the value of the parameters using a whole set of data which are all understood to be potentially generated from a network with one set of parameters. In order to learn the values of the parameters, we need to have some prior distribution over the values the parameters can take. This could be a broad and unrestrictive distribution, expressing little knowledge in the values which the conditional probabilities take. Then we need some approach to using the data to calculate the posterior probability of the parameters. The EM algorithm is a common approach for learning in belief networks. In its standard form it cheats a little as it does not calculate the full posterior probability distribution of the parameters, but rather focusses in on the maximum a posteriori parameter values. Given the problem is a hard one, this is a reasonable start. The EM algorithm works by taking an iterative approach to inference an learning. In the first step, called the E step, the EM algorithm performs inference in the belief network for each of the datum in the dataset. This allows the information from the data to be used, and various necessary statistics S to be calculated from the resulting posterior probabilities. Then in the M step, parameters are chosen to maximise the log posterior log P(T|D,S) given these statistics are fixed. Of course the result is that we now have a new set of parameters, with the result that the statistics S which we collected are no longer accurate. Hence the E step must be repeated, then the M step and so on. At each stage the EM algorithm guarantees that the posterior probability must increase. Hence it eventually converges to a local maxima of the log posterior. This is one way in which conditional probabilities can be learnt from historic data. Having learnt the conditional probabilities these can now be used in the network to make predictions for future data. Hence we have a structured and principled approach for making predictions using a combination of prior knowledge and past data. It is this powerful combination which makes belief networks such a versatile tool. Conclusions Belief Networks are a very general tool in Bayesian statistics. Their combination of structural characteristics with probability theory allows large probability distributions to be handled and manipulated which otherwise would be very cumbersome. Considering belief network structure also allows suitable algorithms for inference to be developed. There is little doubt that the applications of belief networks will continue to grow widely. The purpose of this tutorial is to give a quick and basic and simple introduction to the motivation and inner workings of belief network methods. A Few References There are many good places to look for references on belief networks. For a solid technical introduction and development of exact inference in belief networks, see Pearls orginal book is still worth a read, and introduces the concepts well. Pearl, J. (1988) Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan Kaufmann. Looking at learning, Jordan edited a collection of papers: Jordan, M.I. ed (1998) Learning in Graphical Models. MIT Press. and Heckerman has a good tutorial: Variational Inference and MonteCarlo methods are described in and . Contents Introduction Background Research || Group Publications Demonstrations || Teaching Interests Links Contact email Amos Storkey 
 
 About AUAI Conference Proceedings view online about UAI Mailing List subscribe People Wiki The AUAI Board Upcoming Conference UAI 2014 Conference Web Archive UAI-2013 (Bellevue) UAI-2012 (Catalina Isl.) UAI-2011 (Barcelona) UAI-2010 (Catalina Isl.) UAI-2009 (Montreal) UAI-2008 (Helsinki) UAI-2007 (Vancouver) UAI-2006 (Cambridge) UAI-2005 (Edinburgh) UAI-2004 (Banff) more... About AUAI Conference on Uncertainty in Artificial Intelligence UAI-2014 in Quebec City, Quebec, Canada, July 23 to 27th 2014. Join our Facebook group or add yourself to the UAI Mailing list to keep updated on announcements and relevant AI news. Accessing Proceedings Brightdoc . . The UAI Mailing List To Subscribe Fill out this form. UAI mailing list czar . The UAI Wiki Visit the Previous UAI Conferences Conf # Year Location 29 2013 Bellevue, Washington, USA 28 2012 Catalina Island, California, USA 27 2011 Barcelona, Spain 26 2010 Catalina Island, California, USA 25 2009 Montreal, Canada 24 2008 Helsinki, Finland 23 2007 Vancouver, Canada 22 2006 Cambridge, Massachusetts, USA 21 2005 Edinburgh, Scotland, UK 20 2004 Banff, Canada 19 2003 Acapulco, Mexico 18 2002 Edmonton, Canada 17 2001 Seattle, Washington, USA 16 2000 Stanford, California, USA 15 1999 Stockholm, Sweden 14 1998 Madison, Wisconsin, USA 13 1997 Providence, Rhode Island, USA 12 1996 Portland, Oregon, USA 11 1995 Montreal, Canada 10 1994 Seattle, Washington, USA 9 1993 Washington, DC, USA 8 1992 Stanford, California, USA 7 1991 Los Angeles, California, USA 6 1990 Cambridge, Massachusetts, USA 5 1989 Windsor, Ontario, USA 4 1988 Minneapolis, Minnesota, USA 3 1987 Seattle, Washington, USA 2 1986 Philadelphia, Pennsylvania, USA 1 1985 Los Angeles, California, USA The AUAI Board Directors of AUAI Kathryn Blackmond Laskey , David Poole , Prakash P. Shenoy , Former Directors , Chair (Microsoft Research) (Oregon State University and Cleverset) Finn Jensen Michael Wellman Tod Levitt (C4I Center, George Mason University) John Lemmer Peter Cheeseman Webmaster Mark Crowley , email here 
 Our research Connections Careers About us Microsoft Translator | All Downloads Events Groups News People Projects Publications Videos Our research Reset all Content type Downloads ( 350 ) + Events ( 338 ) Conference ( 123 ) Workshop ( 143 ) Other ( 72 ) Groups ( 147 ) + News ( 2422 ) Feature story ( 485 ) Headline ( 1825 ) Press release ( 112 ) People ( 832 ) Projects ( 994 ) + Publications ( 11273 ) Article ( 1939 ) Inproceeding ( 6832 ) Proceeding ( 594 ) Technical report ( 1908 ) + Videos ( 4711 ) 20th Anniversary Lecture Series 155833 ( 33 ) Microsoft Research Connections 148237 ( 237 ) MSR Talk Series 155418 ( 369 ) Visiting Speakers Series 105128 ( 238 ) 15th Anniversary Celebration Event 105119 ( 2 ) 2012 Microsoft eScience Workshop 174205 ( 25 ) Applied Multi-Party Computation 209400 ( 1 ) AppxFest 189746 ( 6 ) AstroInformatics 2012 172882 ( 13 ) Behind the Code 105112 ( 12 ) Big Data Analytics 193504 ( 10 ) CABI 2013 208037 ( 6 ) Candidate Talk 118214 ( 136 ) CHIME Serial Talk 204515 ( 1 ) Cloud Cryptography Workshop 138062 ( 6 ) Cloud Futures 122473 ( 20 ) Cloud Futures 2012 166273 ( 11 ) Cloud Futures Workshop 2011 150350 ( 15 ) Computational Aspects of Biological Information 105113 ( 8 ) Computational Science Lab 153558 ( 3 ) ConferenceXP 2007 105114 ( 13 ) CoolTalk 173401 ( 1 ) Crypto Colloquium 146116 ( 2 ) Edgenet 2006 105115 ( 16 ) Elliptic Curve Cryptography 25th Anniversary Conference 140503 ( 25 ) Engineering Performance 105116 ( 3 ) eScience 118218 ( 169 ) eScience 2005 138850 ( 29 ) eScience 2008 137316 ( 49 ) eScience 2009 137127 ( 63 ) eScience 2010 142000 ( 35 ) eScience Workshop 2011 157484 ( 15 ) eScience Workshop 2013 206684 ( 26 ) ExCAPE Webinar Series 179954 ( 1 ) eXtreme Computing Group 172113 ( 1 ) F# in Education 141409 ( 3 ) Faculty Summit 2005 154338 ( 22 ) Faculty Summit 2010 139294 ( 39 ) Faculty Summit 2011 152234 ( 27 ) Faculty Summit 2012 169825 ( 38 ) Faculty Summit 2013 197876 ( 43 ) FestSchrift 161550 ( 5 ) FuSe 2013 199048 ( 4 ) Graph Drawing 2012 173001 ( 12 ) History of Parallel Computing 105117 ( 4 ) iConference: Microsoft Campus Visit 145564 ( 2 ) IEEE e-Science Workshop 2011 157481 ( 5 ) IEEE ICCP 2012 163976 ( 7 ) Intern Talk 172670 ( 3 ) Inventing the Future 155809 ( 11 ) Invited Speaker 164001 ( 3 ) Lampsonfest 2014 209380 ( 5 ) LATAM 2011 154771 ( 22 ) Latam 2012 166677 ( 20 ) LATAM 2013 193353 ( 14 ) Machine Learning Day 2013 204322 ( 3 ) Machine Learning Lectures 157243 ( 3 ) Machine Learning Summit 2013 191890 ( 9 ) Machine Learning Workshop 2012 175054 ( 12 ) Memex Summit 105118 ( 14 ) Mind Swap 2013 205503 ( 5 ) MSPAC 105120 ( 5 ) MSR Dev Talks 194466 ( 2 ) MSR India 2012 Summer School on Distributed Algorithms, Systems, and Programming 166581 ( 39 ) MSR India Machine Learning Course 174066 ( 5 ) MSR India Winter School 2012 183394 ( 18 ) MSR Intern Program 171446 ( 3 ) MSR Interview Talk Series 167980 ( 4 ) MSR Symposium on Computational Photography 105121 ( 4 ) MSRI Interview Talk Series 164725 ( 1 ) MSRNE 5th Anniversary Symposium 204675 ( 5 ) Multimedia and Visualization Innovations for Science 145205 ( 4 ) Mysore Park Workshop on Distributed Computing For Machine Learning and Optimization 208912 ( 6 ) Mysore Park Workshop on Machine Learning - 2012 171200 ( 33 ) New Directions in Networked Systems Design 175348 ( 1 ) NIPS 2013 Conference 206938 ( 10 ) NIPS 2013 Tutorials 206971 ( 6 ) Northwest Probability Seminar 140045 ( 3 ) Northwest Regional NLP Workshop 130904 ( 4 ) Oded Schramm Memorial Conference 105122 ( 8 ) Office and Sharepoint Garage 189260 ( 1 ) Open Data for Open Science 2012 162756 ( 14 ) Other 163998 ( 11 ) Pacific Northwest Probability Seminar 204006 ( 4 ) Pacific Northwest Waters Workshop 158982 ( 4 ) Research Next 2013 201538 ( 5 ) Researcher Reflections 158429 ( 19 ) Science Commons Symposium 120375 ( 4 ) Search Summit 105123 ( 13 ) Social Computing Symposium 180167 ( 42 ) Social Computing Symposium 2006 105124 ( 28 ) Software Summit 2011 150063 ( 25 ) Station Q 173828 ( 1 ) STOC 2010 137089 ( 3 ) Summer Number Theory Day 170604 ( 3 ) Tea Talks @ MSR India 171314 ( 3 ) Tech Talk 172673 ( 1 ) TechFest 118220 ( 1 ) TechFest 2007 105125 ( 2 ) TechFest 2011 154953 ( 13 ) TechFest 2012 159785 ( 16 ) TechFest 2013 191271 ( 14 ) TechVista 2013 183941 ( 5 ) TechVista 2014 209012 ( 2 ) TGIF Talk Series 204304 ( 0 ) Theory Group 174688 ( 5 ) Trustworthy Computing (TwC), Privacy 203923 ( 2 ) TwC - Emerging Issues Privacy Summit 206445 ( 1 ) UK Cambridge - Algorithms and Economics 142986 ( 4 ) UK Cambridge - Lab Tutorial 144317 ( 4 ) UK Cambridge - Lecture/Seminar 135987 ( 186 ) UK Cambridge - MLP Talk 140751 ( 25 ) UK Cambridge - MRConnections PhD Summer School 151100 ( 25 ) UK Cambridge - Serious 135988 ( 9 ) UPCRC Multicore Applications Workshop 105126 ( 5 ) UPCRC Workshop 136803 ( 6 ) Virtual Earth Summit 105127 ( 7 ) Visiting Artist Series (Studio99) 207313 ( 2 ) Windows Phone Garage 175952 ( 6 ) Women in SAGE (Software for Algebra and Geometry experimentation) Workshop 142663 ( 2 ) WSHERC 158679 ( 1 ) WW Marketing Excellence 157885 ( 4 ) XAPFest 164154 ( 1 ) More... Labs Asia 47181 ( 1542 ) ATL Cairo 81481 ( 98 ) ATL Europe 81480 ( 95 ) ATL Israel 152623 ( 66 ) Cambridge 47182 ( 3044 ) eXtreme Computing Group 80047 ( 382 ) FUSE Labs 144507 ( 121 ) India 47183 ( 983 ) New England 47184 ( 496 ) New York 163576 ( 118 ) Redmond 47186 ( 9010 ) Silicon Valley 47187 ( 947 ) Station Q 168279 ( 44 ) Research areas Algorithms and theory 47205 ( 660 ) Communication and collaboration 47188 ( 1287 ) Computational linguistics 47189 ( 369 ) Computational sciences 47190 ( 664 ) Computer systems and networking 47191 ( 1836 ) Computer vision 208594 ( 16 ) Data mining and data management 208595 ( 6 ) Economics and computation 47192 ( 264 ) Education 47193 ( 662 ) Gaming 47194 ( 328 ) Graphics and multimedia 47195 ( 1056 ) Hardware and devices 47196 ( 893 ) Health and well-being 47197 ( 369 ) Human-computer interaction 47198 ( 1935 ) Machine learning and intelligence 47200 ( 1380 ) Mobile computing 208596 ( 12 ) Quantum computing 208597 ( 1 ) Search, information retrieval, and knowledge management 47199 ( 1604 ) Security and privacy 47202 ( 659 ) Social media 208598 ( 4 ) Social sciences 47203 ( 747 ) Software development, programming principles, tools, and languages 47204 ( 1286 ) Speech recognition and synthesis 208599 ( 3 ) Technology for emerging markets 208600 ( 22 ) 125 of 21067 Sort Title A to Z Title Z to A Newest to oldest Oldest to newest Most popular 25 | 50 | 100 1 2 3 4 5 6 7 Next Context, Learning, and User Experience for Search Groups James Mickens People LampsonFest - Bob Metcalfe, Bob Sproull, & Alan Kay Videos Windows Azure for Research Training (Tentative) – Mexico City 2014 Events Robust Exact Distance Queries on Massive Networks Publications Windows Azure for Research Training (Tentative) – Seattle 2014 This training is offered to select research community audiences, predominantly consisting of practicing research scientists with at least basic software development skills. The default course consists of two days. Day one helps attendees acquire a general understanding of cloud computing with Windows Azure. Day two focuses on using Windows Azure at scale. Specific instances of the course may revise this basic structure. Related: Groups | People | Projects Microsoft Research Connections Stewart Tansley Windows Azure for Research Event details Date: 1011 September 2014 Location: Seattle, WA, United States Type: Other Luca Cardelli Fest We are holding an event in honour of Luca Cardelli on Monday-Tuesday September 8-9, 2014, at Microsoft Research Cambridge. Details to follow. Related: Groups Programming Principles and Tools Event details Date: 89 September 2014 Location: Cambridge UK Type: Other PRESS: A Novel Framework of Trajectory Compression in Road Networks Renchu Song, weiwei Sun, Baihua Zheng, and Yu Zheng Related: People | Projects Yu Zheng Computing with Spatial Trajectories Publication details Date: 1 September 2014 Type: Inproceedings Distance Oracle on Billion Node Graphs Zichao Qi, Yanghua Xiao, Bin Shao, and Haixun Wang Related: Groups | People Web Search and Data Management Bin Shao Publication details Date: 1 September 2014 Type: Inproceedings Publisher: Very Large Data Bases Endowment Inc. Windows Azure for Research Training (Tentative) – Russia 2014 This training is offered to select research community audiences, predominantly consisting of practicing research scientists with at least basic software development skills. The default course consists of two days. Day one helps attendees acquire a general understanding of cloud computing with Windows Azure. Day two focuses on using Windows Azure at scale. Specific instances of the course may revise this basic structure. Related: Groups | People | Projects Microsoft Research Connections Stewart Tansley Windows Azure for Research Event details Date: 48 August 2014 Location: Russia Type: Other Supporting Distributed Feed-Following Apps over Edge Devices Badrish Chandramouli, Suman Nath, and Wenchao Zhou In feed-following applications such as Twitter and Facebook, users (consumers) follow a large number of other users (producers) to get personalized feeds, generated by blending producers' feeds. With the proliferation of Cloud-connected smart edge devices such as smartphones, producers and consumers of many feed-following applications reside on edge devices and the Cloud. An important design goal of such applications is to minimize communication (and energy) overhead of edge devices. In this paper, we... Related: Groups | People | Projects Database Badrish Chandramouli Suman Nath Streams Publication details Date: 1 August 2014 Type: Inproceedings Scalable Progressive Analytics on Big Data in the Cloud Badrish Chandramouli, Jonathan Goldstein, and Abdul Quamar Analytics over the increasing quantity of data stored in the Cloud has become very expensive, particularly due to the pay-as-you-go Cloud computation model. Data scientists typically manually extract samples of increasing data size (progressive samples) using domain-specific sampling strategies for exploratory querying. This provides them with user-control, repeatable semantics, and result provenance. However, such solutions result in tedious workflows that preclude the reuse of work across samples. On the... Related: Groups | People | Projects Database Badrish Chandramouli Streams Publication details Date: 1 August 2014 Type: Inproceedings PhD Summer School 2014 This annual event is for PhD students in their first or second year from universities and research institutions with which Microsoft Research partners, as well as all Microsoft Research PhD Scholars. It includes a series of talks of academic interest, transferable skills talks, and poster sessions that provide invited students the opportunity to present their work to Microsoft researchers. Related: Groups | People Microsoft Research Connections Scarlet Schwiderski-Grosche Event details Date: 30 June4 July 2014 Location: Cambridge, UK Type: Other Windows Azure for Research Training (Tentative) - Asia 2014 This training is offered to select research community audiences, predominantly consisting of practicing research scientists with at least basic software development skills. The default course consists of two days. Day one helps attendees acquire a general understanding of cloud computing with Windows Azure. Day two focuses on using Windows Azure at scale. Specific instances of the course may revise this basic structure. Related: Groups | People | Projects Microsoft Research Connections Stewart Tansley Windows Azure for Research Event details Date: 910 June 2014 Location: Asia Type: Other Skype Connections and the Gaze of Friendship and Family This is a mini academic conference, sponsored by Skype and MSR, into research on video-mediated communications in private and domestic life. It is particularly interested in the interactional properties of these communications – the forms of talk, gaze and mutual attention rather than the HCI and design aspects (which future events might look at). It will be held on Jun 3rd and 4th at MSR Cambridge. Related: Groups | People Socio-Digital Systems Richard Harper Event details Date: 34 June 2014 Location: Cambridge UK Type: Conference Windows Azure for Research Training (Tentative) – Israel 2014 This training is offered to select research community audiences, predominantly consisting of practicing research scientists with at least basic software development skills. The default course consists of two days. Day one helps attendees acquire a general understanding of cloud computing with Windows Azure. Day two focuses on using Windows Azure at scale. Specific instances of the course may revise this basic structure. Related: Groups | People | Projects Microsoft Research Connections Stewart Tansley Windows Azure for Research Event details Date: 23 June 2014 Location: Israel Type: Other Verification Modulo Versions: Towards Usable Verification Francesco Logozzo, Shuvendu Lahiri, Manuel Fahndrich, and Sam Blackshear Related: Groups | People | Projects | Publications Research in Software Engineering (RiSE) RiSE Working Group on Program Analysis Francesco Logozzo Shuvendu Lahiri Code Contracts SymDiff: Static semantic diff Inference of Necessary Field Conditions with Abstract Interpretation Technology for Inferring Contracts from Code Static contract checking with Abstract Interpretation Publication details Date: 1 June 2014 Type: Inproceedings Publisher: ACM SIGPLAN Analyze This! 145 Questions for Data Scientists in Software Engineering Andrew Begel and Thomas Zimmermann In this paper, we present the results from two surveys related to data science applied to software engineering. The first survey solicited questions that software engineers would like data scientists to investigate about software, about software processes and practices, and about software engineers. Our analyses resulted in a list of 145 questions grouped into 12 categories. The second survey asked a different pool of software engineers to rate these 145 questions and identify the most important ones to... Related: Groups | People | Projects | Publications Empirical Software Engineering Group (ESE) Human Interactions in Programming Research in Software Engineering (RiSE) VIBE Andrew Begel Tom Zimmermann Empirical Studies of Software Engineering Software Process Appendix to Analyze This! 145 Questions for Data Scientists in Software Engineering Analyze This! 145 Questions for Data Scientists in Software Engineering Publication details Date: 1 June 2014 Type: Inproceedings Publisher: ACM Patience is a Virtue: Revisiting Merge and Sort on Modern Processors Badrish Chandramouli and Jonathan Goldstein The vast quantities of log-based data appearing in data centers has generated an interest in sorting almost-sorted datasets. We revisit the problem of sorting and merging data in main memory, and show that a long-forgotten technique called Patience Sort can, with some key modifications, be made competitive with today’s best comparison-based sorting techniques for both random and almost sorted data. Patience sort consists of two phases: the creation of sorted runs, and the merging of these runs. Through a... Related: Groups | People | Projects Database Badrish Chandramouli Streams Publication details Date: 1 June 2014 Type: Inproceedings Publisher: ACM SIGMOD Slicing Probabilistic Programs Chung-Kil Hur, Aditya V. Nori, Sriram K. Rajamani, and Selva Samuel Probabilistic programs use familiar notation of programming languages to specify probabilistic models. Suppose we are interested in estimating the distribution or expectation of a return expression r of a probabilistic program P. We are interested in slicing the probabilistic program P and obtain a simpler program SLI(P) which retains only those parts of P that are relevant to estimating r, and elides those parts P that are not relevant to estimating r. We desire that the SLI transformation be both correct... Related: Groups | People | Projects Programming Languages and Tools Aditya Nori Sriram Rajamani The R2 Project Publication details Date: 1 June 2014 Type: Inproceedings Publisher: ACM EAGLES 2014 - Engineering Automated Games for Learning and Serious Productivity Many aspects of creating, testing and verifying software require huge amounts of manual effort to create models and test cases. This work can be tedious at times, and also difficult to learn and to supervise. Serious games have the potential to make these tasks more interesting and engaging. The process of turning a task or learning activity into a game acts to clarify and regularize the work, making it easier for people to learn and perform the task. Event details Date: 31 May 2014 Location: Hyderabad, India Type: Workshop Windows Azure for Research Training (Tentative) – Mexico City 2014 This training is offered to select research community audiences, predominantly consisting of practicing research scientists with at least basic software development skills. The default course consists of two days. Day one helps attendees acquire a general understanding of cloud computing with Windows Azure. Day two focuses on using Windows Azure at scale. Specific instances of the course may revise this basic structure. Related: Groups | People | Projects Microsoft Research Connections Stewart Tansley Windows Azure for Research Event details Date: 2930 May 2014 Location: Mexico City, Mexico Type: Other Faster Compact Diffie-Hellman: Endomorphisms on the x-line Craig Costello, Huseyin Hisil, and Benjamin Smith Related: People Craig Costello Publication details Date: 11 May 2014 Type: Inproceedings Publisher: Springer Verlag Latin American Faculty Summit 2014 The 2014 Latin American Faculty Summit will bring together thought leaders from academia, government, and Microsoft from a broad range of disciplines including computer science, engineering, mathematics, and economics. Participants will explore the latest advances in computing research, discuss challenges that the community faces, and highlight the best approaches to address those challenges and identify new research opportunities. Related: Groups Microsoft Research Connections Event details Date: 79 May 2014 Location: Viña del Mar, Chile Type: Conference Windows Azure for Research Training (Tentative) – Netherlands 2014 This training is offered to select research community audiences, predominantly consisting of practicing research scientists with at least basic software development skills. The default course consists of two days. Day one helps attendees acquire a general understanding of cloud computing with Windows Azure. Day two focuses on using Windows Azure at scale. Specific instances of the course may revise this basic structure. Related: Groups | People | Projects Microsoft Research Connections Stewart Tansley Windows Azure for Research Event details Date: 78 May 2014 Location: Netherlands Type: Other Windows Azure for Research Training (Tentative) – Germany 2014 This training is offered to select research community audiences, predominantly consisting of practicing research scientists with at least basic software development skills. The default course consists of two days. Day one helps attendees acquire a general understanding of cloud computing with Windows Azure. Day two focuses on using Windows Azure at scale. Specific instances of the course may revise this basic structure. Related: Groups | People | Projects Microsoft Research Connections Stewart Tansley Windows Azure for Research Event details Date: 56 May 2014 Location: Munich, Germany Type: Other Task Specific Continuous Word Representations for Mono and Multi-lingual Spoken Language Understanding Anoop Deoras and Tasos Anastasakos Related: Projects Spoken Language Understanding Publication details Date: 1 May 2014 Type: Proceedings Publisher: IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP) Bootstrapping Privacy Compliance in Big Data Systems Shayak Sen, Saikat Guha, Anupam Dutta, Sriram Rajamani, Janice Tsai, and Jeannette Wing Related: Groups | People Mobility, Networks, and Systems Security and Privacy Saikat Guha Janice Tsai Sriram Rajamani Publication details Date: 1 May 2014 Type: Inproceedings Highly Accurate Phonetic Segmentation Using Boundary Correction Models and System Fusion Andreas Stolcke, Neville Ryant, Vikramjit Mitra, Wen Wang, and Mark Liberman Accurate phone-level segmentation of speech remains an important task for many subfields of speech research. We investigate techniques for boosting the accuracy of automatic phonetic segmentation based on HMM acoustic-phonetic models. In prior work we were able to improve on state-of-the-art alignment accuracy by employing special phone boundary HMM models, trained on phonetically segmented training data, in conjunction with a simple boundary-time correction model. Here we present further improved results... Related: Groups | People Conversational Systems Research Center Andreas Stolcke Publication details Date: 1 May 2014 Type: Inproceedings Publisher: IEEE SPS Curation through use: Understanding the personal value of social media Xuan Zhao and Siân Lindley Content generation on social network sites has been considered mainly from the perspective of individuals interacting with social network contacts. Yet research has also pointed to the potential for social media to become a meaningful personal archive over time. The aim of this paper is to consider how social media, over time and across sites, forms part of the wider digital archiving space for individuals. Our findings, from a qualitative study of 14 social media users, highlight how although some sites... Related: Groups | People | Projects Computer-Mediated Living Socio-Digital Systems Siân Lindley Theme: Human-centred system architectures Publication details Date: 1 May 2014 Type: Inproceedings Publisher: ACM 125 of 21067 Title A to Z Title Z to A Newest to oldest Oldest to newest Most popular 25 | 50 | 100 1 2 3 4 5 6 7 Next Share 2014 Microsoft Contact us Privacy cookies Terms of use Trademarks Code of conduct Feedback Mobile 
 The BAT Project Bat The PATH Project an overview paper The Bat team: Jeff Forbes Tim Huang Keiji Kanazawa Archie Russell Stuart Russell , UC Berkeley Computer Science Michael Cassidy , UC Berkeley Civil Engineering 
 Products Services Fraud Detection Training Sign Up About HUGIN HUGIN EXPERT A/S Home Solutions Products/Services References Technology Case Stories News Archive Company Information Newsletter Hot Links Download Demo Forum Fraud Detection Management Order Form Prices Training Course Contact 中文网站 Cookies Latest News HUGIN Expert A/S  HUGIN Expert A/S is a leading provider of model-based decision support software for reasoning under uncertainty. HUGIN software tools are based on Bayesian Network and influence diagram technology, an advanced statistical technique that supports decision-making in complex domains on the basis of partial, uncertain or unknown information.  Clients in more than 25 countries use HUGIN analytic tools to explore and identify new opportunities, improve operational processes and solve decision problems. HUGIN modeling tools are ideal for stakeholders who want to combine existing data and expertise with the capability to analyze unknown factors and conditions, and in this way uncover valuable insights that can drive new innovation and improve decision-making. Researchers and commercial clients have used HUGIN's Bayesian Network software analytics to develop solutions and applications that must factor uncertainty into their decision-making. Our software has been used in finance, health, agriculture, manufacturing, engineering, communications, energy and forensic science to create real-world solutions for fraud detection, credit default prediction, operational risk management, medical diagnosis, health monitoring, risk analysis, data mining, troubleshooting, safety assessment, forensic identification and more.  Client references      HUGIN FDM – Fraud Detection Management for Insurance Providing automated, intelligent solutions for insurance is a major focus at HUGIN Expert. Using our Fraud Detection Management solution insurers of all sizes can reduce fraud, improve claims handling efficiency, and improve their investigative hit-rates for a fast return on investment.  HUGIN FDM Fraud Detection Management   Unique HUGIN Technology  A Bayesian Network is a probabilistic graphical modeling tool that represents a set of variables and encodes them with conditional dependencies. Because the scope of its intelligence is not constrained by including only known, existing data, but encodes probabilistic relationships among all variables, a Bayesian network is an incredibly powerful decision support tool.  It can uncover future possibilities and opportunities, generate optimal predictions and decisions even when pieces of information are missing or uncertain. This makes Bayesian Network technology a particularly efficient way to deal with the lack of, or ambiguity of information that can limit decision support systems that are based on existing data alone. Learn more about HUGIN's unique Bayesian network technology and probabilistic learning tools .     Visit forum.hugin.com Download the FREE Hugin Lite Training - HUGIN Expert 3-day Bayesian Network Training Course Overcome uncertainty and exploit your data to the fullest using Bayesian Network technology. Join our next 3-day Bayesian Network training course on April 1-3, 2014 and learn how to build efficient decision-making solutions with Bayesian Network tools from HUGIN Expert. Decade-Long Technology Collaboration Nykredit’s decade-long technology collaboration with HUGIN EXPERT has once again resulted in improved business efficiency. HUGIN used for Risk Assessment Since the year 1999 the Danish consultancy group COWI A/S has used HUGIN software to provide advanced decision support in several of its projects. In particular, HUGIN has been used for risk assessment of the decommissioning options related to the removal of offshore production facilities on a number of projects. www.hugin.com 
 Products Services Fraud Detection Training Sign Up About HUGIN HUGIN EXPERT A/S Home Solutions Products/Services References Technology Case Stories News Archive Company Information Newsletter Hot Links Download Demo Forum Fraud Detection Management Order Form Prices Training Course Contact 中文网站 Cookies Latest News HUGIN Expert A/S  HUGIN Expert A/S is a leading provider of model-based decision support software for reasoning under uncertainty. HUGIN software tools are based on Bayesian Network and influence diagram technology, an advanced statistical technique that supports decision-making in complex domains on the basis of partial, uncertain or unknown information.  Clients in more than 25 countries use HUGIN analytic tools to explore and identify new opportunities, improve operational processes and solve decision problems. HUGIN modeling tools are ideal for stakeholders who want to combine existing data and expertise with the capability to analyze unknown factors and conditions, and in this way uncover valuable insights that can drive new innovation and improve decision-making. Researchers and commercial clients have used HUGIN's Bayesian Network software analytics to develop solutions and applications that must factor uncertainty into their decision-making. Our software has been used in finance, health, agriculture, manufacturing, engineering, communications, energy and forensic science to create real-world solutions for fraud detection, credit default prediction, operational risk management, medical diagnosis, health monitoring, risk analysis, data mining, troubleshooting, safety assessment, forensic identification and more.  Client references      HUGIN FDM – Fraud Detection Management for Insurance Providing automated, intelligent solutions for insurance is a major focus at HUGIN Expert. Using our Fraud Detection Management solution insurers of all sizes can reduce fraud, improve claims handling efficiency, and improve their investigative hit-rates for a fast return on investment.  HUGIN FDM Fraud Detection Management   Unique HUGIN Technology  A Bayesian Network is a probabilistic graphical modeling tool that represents a set of variables and encodes them with conditional dependencies. Because the scope of its intelligence is not constrained by including only known, existing data, but encodes probabilistic relationships among all variables, a Bayesian network is an incredibly powerful decision support tool.  It can uncover future possibilities and opportunities, generate optimal predictions and decisions even when pieces of information are missing or uncertain. This makes Bayesian Network technology a particularly efficient way to deal with the lack of, or ambiguity of information that can limit decision support systems that are based on existing data alone. Learn more about HUGIN's unique Bayesian network technology and probabilistic learning tools .     Visit forum.hugin.com Download the FREE Hugin Lite Training - HUGIN Expert 3-day Bayesian Network Training Course Overcome uncertainty and exploit your data to the fullest using Bayesian Network technology. Join our next 3-day Bayesian Network training course on April 1-3, 2014 and learn how to build efficient decision-making solutions with Bayesian Network tools from HUGIN Expert. Decade-Long Technology Collaboration Nykredit’s decade-long technology collaboration with HUGIN EXPERT has once again resulted in improved business efficiency. HUGIN used for Risk Assessment Since the year 1999 the Danish consultancy group COWI A/S has used HUGIN software to provide advanced decision support in several of its projects. In particular, HUGIN has been used for risk assessment of the decommissioning options related to the removal of offshore production facilities on a number of projects. www.hugin.com 
 Heart Disease Program Project Information Group: Clinical Decision Making Group Project Leader: Bill Long wjl@mit.edu Purpose: Description: Questions, comments about this site? Email webmaster@medg.lcs.mit.edu 
 Forside Forskere Publikationer Forskningsprojekter Aktiviteter Presse Forskningsenheder Statistik Om VBN Del Systems for Automatic Customer Support Operations (SACSO) Projekt Med samme deltagere Analysis of MassIve Data STreams - AMIDST Projekt Analysis tools for Bayesian networks and influence diagram Projekt Animated computer games Projekt Anytime hybrid propagation in Bayesian networks Projekt Decision analysis Projekt Extending the scope of graphical models Projekt Inference methods for Bayesian networks and influence diagrams Projekt Learning Bayesian Networks Projekt Medical Decision Support Systems Projekt Modularization of complex tasks Projekt Jensen, Finn Verner (Projektdeltager) Kjærulff, Uffe Bro (Projektdeltager) Olesen, Kristian G. (Projektdeltager) Nielsen, Thomas Dyhre (Projektdeltager) Bangsø, Olav (Projektdeltager) Institut for Datalogi Machine Intelligence Vis graf over relationer Status Afsluttet Periode 01/10/97 → 31/01/01 URL http://www.cs.auc.dk/research/DSS/ Forskningsprogram ingen navn Projekter HP Lab. for normative systems Projekt ID: 16268 © Videnbasen, Aalborg Universitet · vbn@aub.aau.dk · Om VBN Log ind i Pure 
 My favorites Sign in bnt Bayes Net Toolbox for Matlab ProjectHome Downloads Wiki Issues Source Summary People Project Information Project feeds Code license GNU GPL v2 Labels bnt bayesnet graphicalmodel matlab Members murphyk2 mattbdun...@gmail.com 2 committers 1 contributor Links External links PMTK: Probabilistic Modeling Toolkit Bayes Net Toolbox for Matlab Written by Kevin Murphy, 1997--2002. Last updated: 19 October 2007. As on January 2014, a copy of this is available at https://github.com/bayesnet/bnt Major Features Examples of supported Models Download zip file Installation How to use the toolbox Subscribe to the BNT Email List Invited Paper on BNT published in Computing Science and Statistics, 2001. Other Bayes net software A brief introduction to Bayesian Networks Terms and conditions of use (GNU Library GPL) Why do I give the code away? Changelog Why MATLAB? Acknowledgements How do I contribute changes to the code? Terms Privacy Project Hosting Help Powered by Google Project Hosting 

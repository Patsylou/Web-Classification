 Home Overview News Software Publications Links People Partners Documentation Applications HW/SW Resources PDF Wizard visPerf Monitor Documentation Applications Resources Resources HW/SW Resources PDF Wizard visPerf Monitor NetSolve/GridSolve NetSolve/GridSolve is a project that aims to bring together disparate computational resources connected by computer networks. It is a RPC based client/agent/server system that allows one to remotely access both hardware and software components. The purpose of GridSolve is to create the middleware necessary to provide a seamless bridge between the simple, standard programming interfaces and desktop Scientific Computing Environments (SCEs) that dominate the work of computational scientists and the rich supply of services supported by the emerging Grid architecture, so that the users of the former can easily access and reap the benefits (shared processing, storage, software, data resources, etc.) of using the latter. This vision of the broad community of scientists, engineers, research professionals and students, working with the powerful and flexible tool set provided by their familiar desktop SCEs, and yet able to easily draw on the vast, shared resources of the Grid for unique or exceptional resource needs, or to collaborate intensively with colleagues in other organizations and locations, is the vision that GridSolve will be designed to realize. [PDF] Latest NetSolve News 2008-12-18 GridSolve version 0.19.1 released 2008-04-25 GridSolve version 0.18.0 released 2007-07-18 GridSolve version 0.17.0 released 2007-06-03 GridSolve version 0.16.0 released 2006-03-03 Interactive Data Language (IDL) client released Admin Login 

 The OpenMP® API specification for parallel programming OpenMP News Dieter an Mey Appointed to OpenMP Board of Directors February 13, 2014 - Champaign, Illinois - The OpenMP ARB, a group of leading hardware and software vendors and research organizations developing the OpenMP API specification for shared-memory parallelization, appointed Dieter an Mey to its Board of Directors. Dieter brings a wealth of experience as an OpenMP user to the Board. Dieter an Mey leads the High Performance Computing team of the IT Center RWTH Aachen University in Germany. He has a 30+ year track record in HPC with an emphasis on user support and services. Ever since vectorization and message passing and the release of the first OpenMP API specification in 1997, Dieter and his group have actively participated in the OpenMP community. He is co-author of numerous publications on OpenMP programming and productivity. “I believe Dieter will bring a much needed non-commercial viewpoint to the Board, as well as a non-US viewpoint”, said Michael Wong, OpenMP CEO. “He has been a long-time, active OpenMP proponent who is widely respected within the OpenMP community.” Dieter joins Josh Simons of VMware, Sanjiv Shah of Intel, Andy Fritsch of Texas Instruments and Partha Tirumalai of Oracle on the OpenMP Board of Directors. Updated OpenMP 4.0 Examples An update to the OpenMP 4.0 Examples document is now available. It adds new examples that demonstrate  use of the proc_bind clause to control thread binding for a team of threads in a parallel region. Also, new examples for the taskgroup construct. » OpenMP 4.0.1 Examples (PDF) Spring Lang Committee Meeting Scheduled As the Winter face-to-face meeting of the OpenMP Language Committee winds down at Intel in Santa Clara, CA, we can announce the date and location of the next meeting in the Spring: Spring F2F Meeting At: Sidney Sussex College, Cambridge, UK Dates: April 12-16 (Sat-Wed) Hosts: Michael Wong and James Cownie Save the date. Details to follow. Hybrid MPI+OpenMP Tutorial Available Slides and audio from the day-long tutorial on MPI and OpenMP programming presented at Supercomputing 13 in Denver in  November 2013 is now available. » Hybrid MPI and OpenMP Parallel Programming MPI + OpenMP and other models on clusters of SMP nodes - Rolf Rabenseifner, Georg Hager, Gabriele Jost Lang Committee Meeting January 27-31 in Santa Clara The OpenMP Lang Committee will be holding its first face-to-face meeting of 2014 on the Intel campus in Santa Clara, CA January 27-31. This meeting will include breakout sessions and presentations by the various subcommittees ( Accelerator, Error Model, Interoperability, Fortran, Tasking, Examples ) as work continues to develop the next version of the OpenMP API specifications. This meeting is not open to the public without a prior arrangement. Contact the OpenMP ARB at info@openmp.org for more information. Video: Performance Essentials with OpenMP 4.0 Vectorization Intel has posted a video tutorial on the use of the OpenMP 4.0 SIMD pragmas. http://software.intel.com/en-us/articles/performance-essentials-with-openmp-40-vectorization A series of seven videos covering performance essentials using OpenMP 4.0 Vectorization with C/C++. It provides an overview of why explicit vector programming methods are crucial to getting performance on modern many core platforms. In the series, we explore code snippets and background information on such topics as OpenMP* 4.0 Simd-enabled functions, and explicit SIMD loops, as well as techniques to help determine if targeted loops were vectorized and if not, why not. Each video is typically less than 10 minutes in length and yet provides a good starting point for developer who wishes to get started using these technologies. Reflections on SC13 and OpenMP Michael Wong, CEO of OpenMP ARB, reflects on Supercomputing 13 and recent OpenMP advances: I attended Supercomputing in my third year as OpenMP CEO to both represent IBM and OpenMP. This was a big year for us as we closed with many milestones in what I call a Significant Paradigm shift in Parallelism. The most significant milestone was that t he OpenMP Consortium has released OpenMP 4.0 in 2013 with new parallelism features that are productive, portable, and performant across C, C++, and Fortran. OpenMP 4.0 contains significant additions for accelerators, standardized for a broad set of architectures, and an industry-first support for SIMD vectorization. It was being showcased at SC13. The OpenMP ARB Consortium now has 26 members and is still growing, adding three new members in the last year: Red Hat/GCC Barcelona SuperComputing Centre University of Houston Coming implementations of OpenMP 4.0 include GNU, and the Intel 13.1 compiler with support for accelerators. Clang has started with support with OpenMP 3.1. (more) Article: OpenMP 4.0 A new article, Full Throttle: OpenMP 4.0 by Michael Klemm, Senior Application Engineer, Intel  and Christian Terboven, Deputy Head of HPC Group, RWTH Aachen University, appears in the current issue of Intels Parallel Universe magazine. “Multicore is here to stay.” This single sentence accurately describes the situation of application developers and the hardware evolution they are facing. Since the introduction of the first dual-core CPUs, the number of cores has kept increasing. The advent of the Intel® Xeon Phi™ coprocessor has pushed us into the world of manycore— where up to 61 cores with 4 threads each impose new requirements on the parallelism of applications to exploit the capabilities of the hardware. It is not only the ever-increasing number of cores that requires more parallelism in an application. Over the past years, the width of SIMD (Single Instruction Multiple Data) registers has been growing. While the early single instruction multiple data (SIMD) instructions of Intel® MMX™ technology used 64-bit registers, our newest family member, Intel® Advanced Vector Instructions 512 (Intel® AVX-512), runs with 512-bit registers. That’s an awesome 16 floating-point numbers in single precision, or eight double-precision numbers that can be computed in one go. If your application does not exploit these SIMD capabilities, you can easily lose a factor of 16x or 8x compared to the peak performance of the CPU. To read the entire article, download the magazine in PDF .  The article starts on page 6. Previous Entries The OpenMP API Read about OpenMP.org Get OpenMP specs Use OpenMP Compilers Learn Using OpenMP -- the book Using OpenMP -- the examples Using OpenMP -- the forum Wikipedia OpenMP Tutorial More Resources Discuss User Forum Ask the experts and get answers to questions about OpenMP Recent News Dieter an Mey Appointed to OpenMP Board of Directors Updated OpenMP 4.0 Examples Spring Lang Committee Meeting Scheduled Hybrid MPI+OpenMP Tutorial Available Lang Committee Meeting January 27-31 in Santa Clara Subscribe to the News Feed OpenMP Specifications About the OpenMP ARB Frequently Asked Questions Compilers Resources Who's Using OpenMP? Press Releases Videos Discussion Forums Events Public OpenMP Calendar Input Register Follow @OpenMP_ARB Search OpenMP.org Archives February 2014 January 2014 December 2013 November 2013 September 2013 July 2013 May 2013 April 2013 March 2013 February 2013 January 2013 December 2012 November 2012 October 2012 September 2012 July 2012 June 2012 May 2012 April 2012 March 2012 February 2012 January 2012 November 2011 October 2011 September 2011 July 2011 May 2011 February 2011 October 2010 July 2010 May 2010 June 2009 April 2009 March 2009 February 2009 January 2009 November 2008 October 2008 September 2008 August 2008 July 2008 June 2008 May 2008 April 2008 Copyright © 1998-2014. OpenMP and the OpenMP logo are registered trademarks of the OpenMP Architecture Review Board in the United States and other countries. All rights reserved. Trademarks and Logo Usage Privacy Policy 
 Jaguar: Fast Network Communication and I/O in Java J A G U A R Matt Welsh Introduction Jaguar Berkeley Linux VIA JaguarVIA Pre-Serialized Objects Tigris River Ninja Telegraph News May 17, 2000: Jaguar v2.1 has been released! Here is the release documentation Download instructions release docs New in Jaguar v2.1: OpenJIT Jaguar 2 Jaguar bytecode OpenJIT GCJ Software Release Jaguar v2.1 Here is the release documentation Download instructions release docs Papers Safe and Efficient Hardware Specialization of Java Applications (PDF) Jaguar: Enabling Efficient Communication and I/O in Java This paper is the best overall discussion of Jaguar. (PDF) (Gzipped PostScript) Achieving Robust, Scalable Cluster I/O in Java (PDF) A System Supporting High-Performance Communication and I/O in Java (PDF) (Gzipped PostScript) Presentations Jaguar: Bridging the Java Server Performance Gap (HTML) Jaguar: Bridging the Java Server Performance Gap Somewhat out of date, only covers Jaguar 1.0. (HTML) Using Java to Make Servers Scream (HTML) Related Projects UC Berkeley Ninja Project UC Berkeley Telegraph Project OpenJIT Jalapeno Java Grande Forum GCJ 
 Main Page Description FAQ Courses that Use(d) LYDIAN Download Installation Manual html pdf ps.gz Updates Changes Forums A VR Extention EXJOBBS People Related Publications 
 Next: rasit@cs.uno.edu Distributed Shared Memory (DSM) Acknowledgments: 472 entries 34 pages, including some blank pages for double-sided printing Next: Copyright M. Rasit Eskicioglu 
 Parallel Computing 101 Parallel computers are easy to build -- it's the software that takes work. Supercomputing SC13 Abstract Audience slightly whimsical explanation of parallel computing Schedule class on parallel computing Contact: @ www.eecs.umich.edu/~qstout/ Copyright 2005-2013 Quentin F. Stout 
 The Eiffel Parallel Execution Environment PAMPA Project Eiffel EPEE has been used to build EPEE Basic Principles Paladin POM Library Object-Oriented Software Engineering with Eiffel Publications EPEE Team Why Eiffel? 
 The TONS project Current release: 0.0.8 (January 28th) here TONS server node server TAL STALL TASM STC here Jakob Řstergaard Kenneth Geisshirt Jakob stergaard 
 HeNCE (Heterogeneous Network Computing Environment) here PVM hence@cs.utk.edu readme HeNCE-2.0-examples.tgz HeNCE-2.0-src.tgz HeNCE-2.0-doc.ps.gz old-hence hence-2.0-doc-html 
 * and Duke University Table of Contents  What is Proteus? Proteus Publications Proteus Annual Reports 1994 1995  Proteus Software Examples  Proteus Demonstrations  The Proteus Development Group here Mailing lists proteus-users: For users of the Proteus language Additional Information nyland@cs.unc.edu prins@cs.unc.edu times. 
 Welcome to BMDFM The Official Web Site The Site of SMP Dataflow Parallel High Performance Computing and Multi-Core Programming Use Full Power of Your Multiprocessor Machine. Enter the Site Now! Contact: bmdfm@bmdfm.de 2002-2014 by BMDFM - mailto: Webmaster 
 SourceForge Browse Enterprise Blog Help Jobs Log In or Join Solution Centers Go Parallel Smarter IT Resources Newsletters Home Browse Communications Conferencing Invisionix Roaming System Remote metasys prealpha drlean , ilanrab , irsf_admin Summary Files Reviews Support Wiki Tickets ▾ Bugs Support Requests Patches News Discussion Add a Review 3 Downloads (This Week) 2013-04-08 Download irsr-0.2.ZIP Browse All Files Windows Mac Linux Description Your own portable PC system built by integrating existing Open Source components. This mobile metasystem utilizes the internets web hosting resources and is accessed via any web browser enabled appliance from your home, work, school, library, cafes, etc. Invisionix Roaming System Remote metasys Web Site Categories Conferencing , Distributed Computing , Internet , Networking , Symmetric Multi-processing License GNU General Public License version 2.0 (GPLv2) Update Notifications You seem to have CSS turned off. Please don't fill out this field. You seem to have CSS turned off. Please don't fill out this field. Write a Review User Reviews Be the first to post a review of Invisionix Roaming System Remote metasys! Additional Project Details Languages English Intended Audience Developers , Education , End Users/Desktop , Other Audience , System Administrators , Telecommunications Industry User Interface Cocoa (MacOS X) , Handheld/Mobile/PDA , Web-based , Win32 (MS Windows) , X Window System (X11) Programming Language JavaScript , PHP Registered Recommended Projects Corpell vhcs ZK - Simply Ajax and Mobile Loading... The latest tech jobs. See All Jobs Report inappropriate content SourceForge About Site Status @sfnet_ops Find and Develop Software Create a Project Software Directory Top Downloaded Projects Community Blog @sourceforge Job Board Resources Help Site Documentation Support Request Real-Time Support Dice Holdings, Inc. Terms Privacy Opt Out Choices Advertise SourceForge.JP Go Parallel Screenshots can attract more users to your project. Features can attract more users to your project. Upload a new icon Icons must be PNG, GIF, or JPEG and less than 1 MiB in size. They will be displayed as 48x48 images. 
 Harness High Performance Computing Workbench People Publications Software Opportunities Harness is a collaborative computer science research effort of Oak Ridge National Laboratory (ORNL) , the University of Tennessee Knoxville , and Emory University in advanced software solutions for parallel and distributed computing systems with an emphasis on scientific high performance computing (HPC). While past Harness work focused on heterogeneous distributed virtual machine and fault tolerant runtime environments, ongoing research targets optimized scientific application development and deployment processes. The following two paragraphs describe ongoing and past work in more detail. Harness Workbench: Unified and Adaptive Access to Diverse HPC Platforms The goal of this project is to enhance the overall productivity of applications science on diverse high performance computing platforms by conducting research in two innovative software environments. The first is the Harness Workbench Toolkit (HWT) for application building and execution that provides a common view across diverse HPC systems. The HWT consists of a software backplane architecture that presents a uniform but extensible interface for preparatory and pre-execution stages of application execution, which interfaces to instance-specific software via customizable plug-in modules. The second is the next generation Harness runtime environment (HRTE) that similarly provides a flexible, adaptive framework for plugging in modules optimized for a specific HPC system and allows dynamic interfacing to a variety of user environments. Both these environments will employ platform specific pluggable modules disseminating target-specific knowledge and expertise immediately to all end-users who can continue to interface to a familiar environment. Harness: Heterogeneous Distributed Computing The heterogeneous adaptable reconfigurable networked systems (Harness) research project focused on the design and development of a pluggable lightweight heterogeneous Distributed Virtual Machine (DVM) environment, where clusters of PCs, workstations, and ``big iron'' supercomputers can be aggregated to form one giant DVM (in the spirit of its widely-used predecessor, Parallel Virtual Machine (PVM) ). As part of the Harness project, a variety of experiments and system prototypes were developed to explore lightweight pluggable frameworks, adaptive reconfigurable runtime environments, assembly of scientific applications from software modules, parallel plug-in paradigms, highly available DVMs, fault-tolerant message passing ( FT-MPI ), fine-grain security mechanisms, and heterogeneous reconfigurable communication frameworks. Three different Harness system prototypes were developed, two C variants and one Java-based alternative, each concentrating on different research issues. The technology developed within the Harness project influenced many other research and development efforts, such as Open MPI and MOLAR This research is sponsored by the Office of Advanced Scientific Computing Research ; Office of Science ; U.S. Department of Energy . The work is performed jointly at Oak Ridge National Laboratory , which is managed by UT-Battelle, LLC under Contract No. De-AC05-00OR22725, the University of Tennessee Knoxville , and Emory University . Please contact engelmannc@ornl.gov with questions or comments regarding this page. 
 
 Welcome! We are the Parallel Programming Laboratory. Large-scale Parallelization of Stochastic Integer Programs Recent Activity Nikhil receives IBM PhD Fellowship Award Lukasz receives ORNL distinguished software award Akhil et al win Best Paper Award at HiPC 2013 Akhil et al win Two Best Poster Awards at HiPC 2013 Seeking a Visiting Research Programmer Gupta et al. win Best Paper Award at IEEE CloudCom '13 Recent Publications Optimizing Performance Under Thermal and Power Constraints for HPC Data Centers Energy Profile of Rollback-Recovery Strategies in High Performance Computing Overcoming the Scalability Challenges of Epidemic Simulations on Blue Waters Easy, Fast and Energy Efficient Object Detection on Heterogeneous On-Chip Architectures ACM SRC: Structure-Aware Parallel Algorithm for Solution of Sparse Triangular Linear Systems Copyright © 1996-2013 
 Tired of MPI? here 
 Mentat Home Page Mentat Overview Documentation Mentat Team Distribution Sample Programs Slides New Features Legion - The next logical step towards a world-wide virtual computer. Legion is a nationwide metasystem project that uses Mentat as a starting point. Total Access Statistics You are visitor to this page since 10/12/95 Point Survey 

 Current Projects: Multicore Operating Systems Resin UIA System Security Past Projects People Publications Software Search MIT CSAIL Linux scalability systems security publications Support National Science Foundation DARPA Quanta Computer Google Intel ATT IBM 
 Skip to main content. Navigation: Home News Download Publications Contact Us Computing with HTCondor High Throughput Computing (HTC) on large collections of distributively owned computing resources. Guided by both the technological and sociological challenges of such a computing environment, the Center for High Throughput Computing at UW-Madison has been building the open source HTCondor distributed computing software until its name changed here is scheduled for April 28-30, 2014 Latest News Consider presenting your work at HTCondor Week 2014 February 20, 2014 Information for HTCondor Week Speakers HTCondor 8.0.6 released! February 11, 2014 Version History Downloads HTCondor Week 2014 Announced: April 28-30 January 10, 2014 HTCondor Week 2014 Talks and tutorials on new HTCondor features Talks on future plans for HTCondor Introductory tutorials on using and administrating HTCondor The opportunity to meet with HTCondor developers and other users HTCondor 8.1.3 released! December 23, 2013 Version History Downloads HTCondor 8.0.5 released! December 12, 2013 Version History Downloads HTCondor assists in Einstein@Home discovery of a new radio pulsar November 4, 2013 We add our congratulations to James Drews of the University of Wisconsin - Madison for the discovery of J1859+03 , a new radio pulsar, within data from the Arecibo Observatory PALFA survey. James backfills the Computer Aided Engineering HTCondor 8.1.2 released! October 31, 2013 Version History Downloads HTCondor 8.0.4 released! October 24, 2013 Version History Downloads More News Software What is the HTCondor software? Downloads including HTCondor source and binaries for Linux, Windows, and Mac. Documentation primarily consists of the HTCondor Manual . Also see HOWTO recipes and job submission examples . Support Options include community and professional solutions. Forthcoming release plans License Information Security Information Community Email Lists - ask questions and share experiences with users worldwide. HTCondor Week is our annual community meeting. Materials from past meetings include talks from science and industry, plus useful tutorials. HTCondor Wiki contains FAQs , bug tickets, and more. Live chat via IRC on Freenode #distcomp Blog Posts aggregated at GitHub from multiple sites. Tutorials Privacy Notice Official Logos Research and Development Research Publications HTCondor Technologies including the widely used ClassAd Library Information for Developers Jobs available in support of distributed computing at UW-Madison The CHTC Team at the UW-Madison, and a list of code contributors and maintainers. email htcondor-admin@cs.wisc.edu 
 This page uses frames, but your browser doesn't support them. 
 Skip to content Skip to main navigation Skip to first column Skip to second column The Design of Open Egineering Systems Lab University at Buffalo - The State University of New York Welcome to the Design of Open Engineering Systems (DOES) Research Laboratory web page. Please feel free to browse through the lab member profiles to learn more about our specific research topics, or to visit the current openings page to view available research positions. At the DOES laboratory, research is being conducted to promote and advance the state-of-the-art in multidisciplinary design optimization and modern design theory. Current research deals with multidimensional visualization of uncertainty, reconfigurable systems, mass customization, and decision making techniques. We have an extensive list of publications that are available. MAE DEPARTMENT The DOES Lab is part of the Mechanical and Aerospace Engineering Department here at the University at Buffalo. Please feel free to visit the departments website to learn more about the department in general. MAE Department Website NYSCEDII The DOES Lab is affiliated with The New York State Center for Engineering Design and Industrial Innovation (NYSCEDII). Please feel free to visit the NYSCEDII website to learn more about them. NYSCEDII Website Main Menu Home People Publications Contact Us Search Login Username Password Remember me Forgot login? 
 University of Utah Avalanche Low communication latency Message Passing and Distributed Shared Memory Cache and Communication Controller Unit Hewlett-Packard PA Myricom Inc ARPA CSTO SPAWAR 
 Welcome to the World Wide Web home page for the D System, Rice University Objective machine-independent , data-parallel To find out more... dsystem-info@cs.rice.edu Other pages on the project and the group's other research: 
 Virtual memory-mapped communication (VMMC) was developed out of the need for a basic multicomputer communication mechanism with extremely low latency and high bandwidth. This is achieved by allowing applications to transfer data directly between two virtual memory address spaces over the network. The basic mechanism is designed to efficiently support applications and common communication models such as message passing, shared memory, RPC, and client-server. Experience with implementing connection-oriented communication using VMMC exposed some deficiencies in the VMMC model. VMMC-2 was designed to overcome those deficiencies. VMMC-2 extends VMMC with three mechanisms: a user-managed TLB mechanism for address translation which enables user libraries to dynamically manage the amount of pinned space and requires only driver support from many operating systems, a transfer redirection mechanism which allows to avoid a copy on the receiver's side, and a reliable communication protocol at the data link layer which allows to avoid a copy on the sender's side. Last Updated : March 22, 1999 skumar@cs.princeton.edu 
 Home Papers People News Positions Links Contact 1 paper in ASPLOS-2014 3 papers in HPCA-2014 1 paper in MICRO-2013 The i-acoma Architecture Group Professor Josep Torrellas The Bulk Multicore Architecture [Presentation slides]. Thrifty: An Extreme-Scale Multiprocessor Architecture [Presentation slides]. UHPC Speculative multithreading Low-power design Old Projects Learn about our papers In chronological order: Full List Multiprocessor Organization and System Design Speculative Multithreading Hardware Reliability and Variability Support for Software Reliability Low-Power Design Processor-Memory Integration Prefetching and Forwarding Databases and Operating Systems Tools and Misc Learn about us: Leader: Josep Torrellas Current Students Research Scientists Associated to the Project Already Graduated Contact torrellas@cs.uiuc.edu with questions or comments on this page. blogger template. 
 Wisconsin Multifacet Project UW-Madison Computer Architecture Group Department of Computer Sciences University of Wisconsin-Madison Mark Hill David Wood Overview People Publications & Talks Theses Related Projects Links Overview Publications & Talks By Year 2013 2012 2011 2010 2009 2008 2007 2006 2005 2004 2003 2002 2001 2000 1999 1998 By Topic Multiprocessor Design, Specification, & Verification Transactional Memory LogTM Home Page ) Deterministic Recording & Replay Modeling & Simulation Workload Analysis & Improvement Availability Miscellaneous Software & Tools The gem5 Simulator System Multifacet's General Execution-driven Multiprocessor Simulator (GEMS) Sponsors Award Number 9971256 Award Number 0205286 Award Number 0324878 Award Number 0551401 Award Number 0720565 Award Number 0916725 Award Number 1017650 Google HP (formerly Compaq Computer and Digital Equipment Corporations) IBM Intel Microsoft Oracle (formerly Sun Microsystems) Sandia National Laboratories The project that spawned this project Wisconsin Wind Tunnel Project 
 hankd@ecn.purdue.edu January 1995 here gif ( n K .ps.Z) here [SiN87] any or all [DiO92] [DiC94] [Jor78] wait wait [SiN87] [OKD90] [OKD90a] [CoD94a] [DiO92] i 's bit i wait [Cra93] ( P_PORTBASE P_PORTBASE LPT1: , LPT2: , and LPT3: P_PORTBASE P_PORTBASE P_PORTBASE ( ( [CoD94] [CoD94b] [OKD90a] [OKD90] [DiO92] [CoD94a] iopl() if (iopl(3)) { /* iopl failed, implying we were not priv */ exit(1); } all nice() ioperm() if (ioperm(P_PORTBASE, 3, 1)) { /* like iopl, failure implies we were not priv */ exit(1); } P_PORTBASE if (ioperm(P_PORTBASE, 2, 1)) { /* like iopl, failure implies we were not priv */ exit(1); } inline unsigned int inb(unsigned short port) { unsigned char _v; __asm__ __volatile__ ("inb %w1,%b0" :"=a" (_v) :"d" (port), "0" (0)); return(_v); } inline void outb(unsigned char value, unsigned short port) { __asm__ __volatile__ ("outb %b0,%w1" : /* no outputs */ :"a" (value), "d" (port)); } P_PORTBASE /* To output to P_PORTBASE */ #define P_OUT(x) \ outb(((unsigned char)(x)), \ ((unsigned short) P_PORTBASE)) /* To input from P_PORTBASE+1 */ #define P_IN() \ inb((unsigned short) (P_PORTBASE + 1)) /* To output to P_PORTBASE+2 */ #define P_MODE(x) \ outb(((unsigned char)(x)), \ ((unsigned short) (P_PORTBASE + 2))) P_RDY bit, and not P_INT P_MODE(P_NAK); P_OUT(last_out ^= (P_S0 | P_S1)); last_out P_S0 P_S1 /* Which condition am I waiting for? */ if (last_out P_S0) { /* Waiting for P_RDY */ if ((!(P_IN() P_RDY)) (!(P_IN() P_RDY))) { /* Polled twice, make LED red */ P_OUT(last_out ^= (P_LG | P_LR)); /* Continue waiting */ while (!(P_IN() P_RDY)) CHECKINT; /* Ok, LED green again */ P_OUT(last_out ^= (P_LG | P_LR)); } } else { /* Waiting for not P_RDY */ if ((P_IN() P_RDY) (P_IN() P_RDY)) { /* Polled twice, make LED red */ P_OUT(last_out ^= (P_LG | P_LR)); /* Continue waiting */ while (P_IN() P_RDY) CHECKINT; /* Ok, LED green again */ P_OUT(last_out ^= (P_LG | P_LR)); } } CHECKINT CHECKINT { /* Make P_INT status visible */ P_MODE(P_SEL | P_NAK); /* Check for interrupt */ if (P_IN() P_INT) { /* Process the interrupt.... */ } else { /* Restore P_RDY */ P_MODE(P_NAK); } } P_RDY P_RDY x last_out = ((last_out 0xf0) | x ); nand _ result = P_IN(); nand _ result (( nand _ result 3) 0x0f) ( [BrG94] [BrG94] why Why Why Why Why http://garage.ecn.purdue.edu/~papers/ [BrG94] 8th International Parallel Processing Symposium [CoD94] Dynamic Barrier Architecture For Multi-Mode Fine-Grain Parallelism Using Conventional Processors; Part I: Barrier Architecture, [CoD94a] Dynamic Barrier Architecture For Multi-Mode Fine-Grain Parallelism Using Conventional Processors; Part II: Mode Emulation, [CoD94b] Proc. of 1994 Int'l Conf. on Parallel Processing, [Cra93] Cray T3D System Architecture Overview [DiC94] 7th Annual Workshop on Languages and Compilers for Parallel Computing [DiM94] PAPERS: Purdue's Adapter for Parallel Execution and Rapid Synchronization [DiO92] The Journal of Supercomputing [Jor78] Proc. Int'l Conf. on Parallel Processing [OKD90] Proc. of 1990 Int'l Conf. on Parallel Processing, [OKD90a] Proc. of 1990 Int'l Conf. on Parallel Processing, [SiN87] Proc. of Second Int'l Conf. on Supercomputing, Abstract Keywords Notice for HTML Users 1. Introduction 1.1. Synchronization 1.2. Communication 1.3. Interrupts 2. PC Hardware 2.1. PE Hardware Interface 2.2. PE Port Bit Assignments 3. TTL_PAPERS Hardware 3.1. Barrier/Interrupt Hardware 3.2. Aggregate Communication Hardware 3.3. LED Display Hardware 4. PAPERS Software 4.1. Operating System Interface 4.1.1. Generic UNIX 4.1.2. Linux 4.2. Port Access 4.3. Barrier Interface 4.4. NAND Data Communication 5. Performance 6. Conclusion References Hypertext Index The only 
 The current system specifications are: GrangeNet. EnFuzion Copyright 1998-2004 Monash University 
 start UC Berkeley Clustered Computing Trace: start Table of Contents Overview Cluster Resources Contact Overview UC Berkeley Department of Electrical Engineering and Computer Sciences . Anyone with an EECS acccount System News Tue 27 Aug 2013 - Free Beta Test of queues on new zen headnode Tue 6 Aug 2013 - Reboot S132 at 10:30am; back at 11:15am. NFS was hung, rendering /work, /usr/mill and /usr/sww inaccessible. Wed 24 Jul 2013 - S131 crashed. /work4 inaccessible. rebooting at 9:45am. Fri 26 Apr 2013 - Failure of primary NIS and DNS server caused temporary access issues Mon 18 Mar 2013 - Network failure partitioning research net has been patched Wed 06 Mar 2013 - Ganglia ( http://monitor.millennium.berkeley.edu/ ) is down. Sat 15 Dec 2012 - a DNS server, tangelo, went down this morning, resulting in slow/unreliable logins to many systems - rebooted, service restored Fri 30 Nov 2012 - /work server hung (also /usr/mill and /usr/sww) - rebooted and remounted on cluster Thu 29 Nov 2012 - Power failure in Soda Hall resulted in complete cluster restart Sat 9 Jun 2012 - DNS server failed, took down everything with it, recovering. Mon 29 Aug 2011 - /work2 and /work4 down due to hardware failure - /work2 back, /work4 data migrated Tue Jul 19 2011 - planned power shutdown in Sutardja Dai Hall on Thu Jul 21 will affect cluster users Sun May 8 2011 - s132, which serves /work for the PSI Cluster and some other systems and /usr/(mill|sww) for all systems, crashed. Its back up. Thu Jan 27 2011 - Blackbox is offline for unknown reasons Cluster Resources EECS Clusters PSI Cluster (x64) i3 Cluster (x86) Private Clusters Nano Cluster (ppc64) NLP Cluster (x86) RAD Lab Cluster (x64) Sensor Network Testbeds Account Request Form Documentation MPI Documentation Torque Batch Scheduler Gexec Documentation Matlab Retired CITRIS Cluster (ia64) Millennium Cluster (x86) Oceanstore Cluster (x86) Contact Name Phone Email Jeff Anderson-Lee 3-6447 Eric Fraser 2-8698 support at Millennium.Berkeley.EDU Albert Goto 3-7472 Jon Kuroda 3-7455 internal Except where otherwise noted, content on this wiki is licensed under the following license: CC Attribution-Noncommercial-Share Alike 3.0 Unported 
 About PC² Annual Report 2010/11 (PDF) Start page About PC² Contact How to reach us Car Train Air Hotel recommendations History News, Talks & Jobs News Talks Events ZKI SC 2013 Agenda Tagungsort Hotels Abendveranstaltung Ansprechpartner Job offers Research Custom Computing and Many-Cores Middleware Scalable Storage Systems Testbeds and Benchmarking Cooperation partners Opensource Projects Staff & Board Management Board Advisory Board Staff Students Publications HPC Systems & Services Offered services HPC Systems D-Grid Resources RV-NRW Registration Cloud/Grid Services RegAuth ServiceCatalog Preface Systems Applications Cloud/Grid Consulting External Courses Clauses User Directories and Filesystems Available systems HPC Cluster HPC Cloud Pling2 ARMINIUS+ News How To FAQ System Configuration BisGrid Cluster BisGrid Cluster News HTC Principal Workflow Architecture Installed Software Restrictions FAQs SMP Server Big-Data-Cluster Available Software Teaching Bachelor & Master theses Lectures Language: Welcome to PC² The Paderborn Center for Parallel Computing, PC², is an interdisciplinary institute of the University of Paderborn, Germany. We are specialized in distributed and parallel computing for research, development and practical applications. New emerging fields of applications are investigated together with our partners. Practical work is done in a number of different areas shown on our research and project web pages. Our parallel computers are amongst the most powerful of their type. They enable us to study practical applications in a high-performance computing environment. Among our computing facilities are several large InfiniBand-clusters with up to 10.000 processor cores, and some smaller machines. These systems can be accessed conveniently by all users via the X-WIN network. Within the supporting groups, theoretical work is done to develop methods and principles for the construction and efficient use of distributed and parallel computer systems . Latest System News Posted on 22 March 2013 in System News: New Cluster arrived: "OCuLUS" Click here to learn more about OCuLUS. Latest News Posted on 04 December 2013 in Talks: Services des PC2: Status und zukünftige Entwicklung 19 December 2013, 16:00 in O2.267 (Gebäude O) Posted on 15 August 2013 in General News: Silver Medal for OCuLUS User at Olympic Computer Games in Computer GO Lars Schäfers, Tobias Graf Posted on 19 July 2013 in Talks: Accelerating Lattice QCD simulations with GPU clusters 24 July 2013, 13:00 in Hörsaal O2 (Gebäude O) Dr. Mathias Wagner, Fakultät für Physik, Universität Bielefeld Copyright 2008-2011 Paderborn Center for Parallel Computing Imprint Sitemap 
